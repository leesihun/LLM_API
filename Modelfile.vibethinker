# Modelfile for VibeThinker-1.5B
# A 1.5B parameter dense language model optimized for mathematical reasoning and competitive coding
# Base: Qwen2.5-Math-1.5B | Training Cost: $7,800 | Performance: Surpasses DeepSeek R1 on math benchmarks
# License: MIT | Source: WeiboAI (Weibo Corporation)

# =============================================================================
# BASE MODEL CONFIGURATION
# =============================================================================
# NOTE: Replace this with the actual GGUF model path once you've converted/downloaded it
# Example conversion: python convert_hf_to_gguf.py /path/to/WeiboAI/VibeThinker-1.5B
FROM ./models/vibethinker-1.5b-q4_K_M.gguf

# =============================================================================
# MODEL PARAMETERS - Optimized for Reasoning Tasks
# =============================================================================
# Based on official recommendations from WeiboAI:
# - Temperature: 0.6 for balanced reasoning, 1.0 for creative exploration
# - Max tokens: 40960 (supports long-form reasoning chains)
# - Top_p: 0.95 (nucleus sampling for quality)
# - Top_k: -1 (disabled, relying on top_p instead)

# Temperature: 0.6 provides balanced reasoning with controlled randomness
# Use 1.0 for more diverse/creative problem-solving approaches
PARAMETER temperature 0.6

# Context window: 40960 tokens to support extended reasoning chains
# VibeThinker excels at chain-of-thought reasoning requiring substantial context
PARAMETER num_ctx 40960

# Top-p (nucleus sampling): 0.95 for high-quality token selection
PARAMETER top_p 0.95

# Top-k: Disabled (-1) as recommended by WeiboAI
# The model performs better with pure top_p sampling
PARAMETER top_k -1

# Repeat penalty: Mild penalty to reduce repetition while preserving reasoning flow
PARAMETER repeat_penalty 1.05

# Number of tokens to penalize for repetition
PARAMETER repeat_last_n 128

# Stop sequences: Common reasoning completion markers
PARAMETER stop "<|im_end|>"
PARAMETER stop "<|endoftext|>"

# =============================================================================
# SYSTEM PROMPT - Optimized for Mathematical & Coding Reasoning
# =============================================================================
SYSTEM """You are VibeThinker-1.5B, a specialized AI assistant optimized for mathematical reasoning and competitive programming problems.

Core Capabilities:
- Advanced mathematical problem-solving (algebra, calculus, geometry, combinatorics)
- Competitive programming and algorithm design
- Step-by-step logical reasoning with clear explanations
- Code generation in Python, C++, and other programming languages

Reasoning Approach:
1. Carefully analyze the problem to identify key constraints and objectives
2. Break down complex problems into manageable sub-problems
3. Apply systematic reasoning, showing your thought process
4. Verify your solution for correctness and edge cases
5. Provide clear, concise explanations alongside your answers

Response Format:
- For math problems: Show detailed step-by-step work
- For coding problems: Explain your algorithm, then provide clean, efficient code
- For explanations: Use structured reasoning with logical flow
- Always double-check calculations and code logic before responding

Remember: You excel at competitive-style problems. Take your time to think through challenges methodically and provide high-quality, accurate solutions.
"""

# =============================================================================
# CHAT TEMPLATE - Qwen2.5 Format
# =============================================================================
# VibeThinker uses Qwen2.5 chat template format
# Template structure: <|im_start|>role\ncontent<|im_end|>
TEMPLATE """{{- if .System }}
<|im_start|>system
{{ .System }}<|im_end|>
{{ end }}
{{- if .Prompt }}
<|im_start|>user
{{ .Prompt }}<|im_end|>
{{ end }}
<|im_start|>assistant
{{ .Response }}<|im_end|>
"""

# =============================================================================
# MODEL METADATA
# =============================================================================
# License information
LICENSE MIT

# Model information for display
MESSAGE """
VibeThinker-1.5B - High-Performance Reasoning Model

Architecture: 1.5B dense transformer (Qwen2.5-Math-1.5B base)
Specialization: Mathematical reasoning & competitive programming
Performance: Surpasses 600B+ parameter models on AIME24/25, HMMT25 benchmarks
Training Cost: $7,800 (30-60x more efficient than comparable models)

Recommended Use Cases:
✓ Mathematical problem-solving (competition math, proofs, calculations)
✓ Competitive programming challenges (LeetCode, Codeforces, etc.)
✓ Algorithm design and optimization
✓ Code generation with logical reasoning
✓ Step-by-step analytical tasks

Configuration:
- Temperature: 0.6 (balanced) | Use 1.0 for creative exploration
- Max Tokens: 40,960 (extended reasoning support)
- Optimized for chain-of-thought reasoning

Source: WeiboAI/VibeThinker-1.5B
Documentation: https://github.com/WeiboAI/VibeThinker
License: MIT (Free for commercial use)
"""

# =============================================================================
# USAGE INSTRUCTIONS
# =============================================================================
# To create this model in Ollama:
# 1. Convert the HuggingFace model to GGUF format (or download pre-quantized version)
# 2. Update the FROM path above to point to your GGUF file
# 3. Run: ollama create vibethinker -f Modelfile.vibethinker
# 4. Test: ollama run vibethinker "Solve: If x^2 + y^2 = 25 and x + y = 7, find x*y"
#
# For optimal performance with competitive math/coding:
#   ollama run vibethinker --temperature 0.6 --num-ctx 40960
#
# For more creative/diverse solutions:
#   ollama run vibethinker --temperature 1.0 --num-ctx 40960
#
# Recommended quantization: Q4_K_M or Q5_K_M for best quality/performance balance
# =============================================================================
