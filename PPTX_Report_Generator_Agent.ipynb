{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-Orchestrated PowerPoint Report Generator\n",
    "\n",
    "This notebook uses **your LLM API's agentic capabilities** to automatically generate comprehensive PowerPoint reports from warpage data.\n",
    "\n",
    "## How It Works\n",
    "\n",
    "Instead of hardcoding visualizations and slides, this notebook:\n",
    "1. **Uploads your data** to the LLM API\n",
    "2. **Sends strategic prompts** to the ReAct agent\n",
    "3. **Lets the agent autonomously**:\n",
    "   - Analyze the data\n",
    "   - Generate visualization code\n",
    "   - Execute the code\n",
    "   - Create PowerPoint slides\n",
    "   - Add charts and formatting\n",
    "\n",
    "**Key Advantage:** The AI adapts to your data structure, fixes errors automatically, and can suggest additional insights.\n",
    "\n",
    "**Technology Stack:**\n",
    "- Your LLM API (ReAct agent + python_coder tool)\n",
    "- python-pptx (auto-generated)\n",
    "- matplotlib/seaborn (auto-generated)\n",
    "\n",
    "**Supports:** Multiple datasets, automatic comparison, adaptive visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration\n",
    "\n",
    "USE THIS AS IT IS, NO NEED TO CHANGE!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# LLM API Client (from API_examples.ipynb)\n",
    "class LLMApiClient:\n",
    "    def __init__(self, base_url: str, timeout: float = 3600.0):\n",
    "        self.base_url = base_url.rstrip(\"/\")\n",
    "        self.token = None\n",
    "        self.timeout = httpx.Timeout(50.0, read=timeout, write=timeout, pool=timeout)\n",
    "\n",
    "    def _headers(self):\n",
    "        h = {}\n",
    "        if self.token:\n",
    "            h[\"Authorization\"] = f\"Bearer {self.token}\"\n",
    "        return h\n",
    "\n",
    "    def login(self, username: str, password: str):\n",
    "        r = httpx.post(f\"{self.base_url}/api/auth/login\", json={\n",
    "            \"username\": username, \"password\": password\n",
    "        }, timeout=10.0)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        self.token = data[\"access_token\"]\n",
    "        return data\n",
    "\n",
    "    def list_models(self):\n",
    "        headers = {\"Authorization\": f\"Bearer {self.token}\"} if self.token else {}\n",
    "        r = httpx.get(f\"{self.base_url}/v1/models\", headers=headers, timeout=10.0)\n",
    "        r.raise_for_status()\n",
    "        return r.json()\n",
    "\n",
    "    def chat_new(self, model: str, user_message: str, agent_type: str = \"auto\", files: list = None):\n",
    "        messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "        data = {\n",
    "            \"model\": model,\n",
    "            \"messages\": json.dumps(messages),\n",
    "            \"agent_type\": agent_type\n",
    "        }\n",
    "        \n",
    "        files_to_upload = []\n",
    "        if files:\n",
    "            for file_path in files:\n",
    "                f = open(file_path, \"rb\")\n",
    "                files_to_upload.append((\"files\", (Path(file_path).name, f)))\n",
    "        \n",
    "        try:\n",
    "            r = httpx.post(\n",
    "                f\"{self.base_url}/v1/chat/completions\",\n",
    "                data=data,\n",
    "                files=files_to_upload if files_to_upload else None,\n",
    "                headers=self._headers(),\n",
    "                timeout=self.timeout\n",
    "            )\n",
    "            r.raise_for_status()\n",
    "            result = r.json()\n",
    "            return result[\"choices\"][0][\"message\"][\"content\"], result[\"x_session_id\"]\n",
    "        finally:\n",
    "            for _, (_, f) in files_to_upload:\n",
    "                f.close()\n",
    "\n",
    "    def chat_continue(self, model: str, session_id: str, user_message: str, agent_type: str = \"auto\", files: list = None):\n",
    "        messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "        data = {\n",
    "            \"model\": model,\n",
    "            \"messages\": json.dumps(messages),\n",
    "            \"session_id\": session_id,\n",
    "            \"agent_type\": agent_type\n",
    "        }\n",
    "        \n",
    "        files_to_upload = []\n",
    "        if files:\n",
    "            for file_path in files:\n",
    "                f = open(file_path, \"rb\")\n",
    "                files_to_upload.append((\"files\", (Path(file_path).name, f)))\n",
    "        \n",
    "        try:\n",
    "            r = httpx.post(\n",
    "                f\"{self.base_url}/v1/chat/completions\",\n",
    "                data=data,\n",
    "                files=files_to_upload if files_to_upload else None,\n",
    "                headers=self._headers(),\n",
    "                timeout=self.timeout\n",
    "            )\n",
    "            r.raise_for_status()\n",
    "            result = r.json()\n",
    "            return result[\"choices\"][0][\"message\"][\"content\"], result[\"x_session_id\"]\n",
    "        finally:\n",
    "            for _, (_, f) in files_to_upload:\n",
    "                f.close()\n",
    "\n",
    "# Configuration\n",
    "API_BASE_URL = 'http://localhost:1007'\n",
    "USERNAME = \"leesihun\"\n",
    "PASSWORD = \"s.hun.lee\"\n",
    "\n",
    "# Initialize client\n",
    "client = LLMApiClient(API_BASE_URL, timeout=3600.0)\n",
    "print(f\"Client initialized: {API_BASE_URL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login and get model\n",
    "client.login(USERNAME, PASSWORD)\n",
    "models = client.list_models()\n",
    "MODEL = models[\"data\"][0][\"id\"]\n",
    "\n",
    "print(f\"Logged in as: {USERNAME}\")\n",
    "print(f\"Using model: {MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure Data Files\n",
    "\n",
    "**Edit this cell** to specify your warpage statistics files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# CONFIGURATION: Define your data files\n",
    "# ========================================\n",
    "\n",
    "# Option 1: Single file\n",
    "# stats_paths = [Path(f\"data/uploads/{USERNAME}/20251013_stats.json\")]\n",
    "\n",
    "# Option 2: Multiple files for comparison\n",
    "stats_paths = [\n",
    "    Path(f\"B8_1021_stats.json\"),\n",
    "    Path(f\"B8_1027_stats.json\"),\n",
    "]\n",
    "\n",
    "# Verify files exist\n",
    "print(f\"Configured {len(stats_paths)} data file(s):\\n\")\n",
    "\n",
    "\n",
    "for i, path in enumerate(stats_paths, 1):\n",
    "    if path.exists():\n",
    "        size_kb = path.stat().st_size / 1024\n",
    "        print(f\"  [{i}] {path.name} ({size_kb:.1f} KB) - OK\")\n",
    "    else:\n",
    "        print(f\"  [{i}] {path.name} - FILE NOT FOUND\")\n",
    "\n",
    "# Convert to strings for API\n",
    "file_paths_str = [str(p) for p in stats_paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Phase 1: Data Analysis (AI-Driven)\n",
    "\n",
    "Let the AI agent analyze your data and recommend visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_prompt = f\"\"\"\n",
    "I have {len(stats_paths)} warpage measurement dataset(s) attached as JSON files.\n",
    "\n",
    "TASK: Comprehensive Data Analysis\n",
    "\n",
    "Please perform the following analysis:\n",
    "\n",
    "1. **Calculate Key Statistics:**\n",
    "   - Total number of measurements across all datasets\n",
    "   - Overall mean, std, min, max, range\n",
    "   - Number of outliers by viewing PCA values as shown in pc1, pc2\n",
    "   - Dataset comparison between different production dates\n",
    "\n",
    "2. **Identify Visualization Needs:**\n",
    "   - What trends should be visualized?\n",
    "   - What comparisons are important?\n",
    "   - What outliers or anomalies should be highlighted?\n",
    "\n",
    "3. **Return Summary:**\n",
    "   Provide a clear summary with:\n",
    "   - Which data is better and why?\n",
    "   - Any concerns or interesting patterns found\n",
    "\n",
    "Be specific and data-driven in your recommendations.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE 1: DATA ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nUploading {len(stats_paths)} file(s) to AI agent...\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "analysis_result, session_id = client.chat_new(\n",
    "    MODEL,\n",
    "    analysis_prompt,\n",
    "    agent_type=\"auto\",  # Let agent decide (will likely use python_coder)\n",
    "    files=file_paths_str\n",
    ")\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\nAnalysis completed in {elapsed:.1f}s\\n\")\n",
    "print(\"=\" * 80)\n",
    "print(\"AI ANALYSIS RESULT\")\n",
    "print(\"=\" * 80)\n",
    "print(analysis_result)\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Phase 2: Generate Visualizations (AI-Driven)\n",
    "\n",
    "Let the AI agent create all visualizations based on the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization_prompt = f\"\"\"\n",
    "Based on the previous data analysis, now generate comprehensive visualizations.\n",
    "\n",
    "TASK: Create High-Quality Visualization Charts\n",
    "\n",
    "Create the following {8 if len(stats_paths) == 1 else 9} professional visualizations:\n",
    "\n",
    "**Required Charts:**\n",
    "\n",
    "1. **Temporal Trends** (temporal_trends.png)\n",
    "   - Line chart: mean values over file index (temporal sequence)\n",
    "   - Add median line (dashed)\n",
    "   - Shade ±1 standard deviation area\n",
    "   - Labels: 'File Index', 'Warpage Value'\n",
    "\n",
    "{'''2. **Dataset Comparison** (dataset_comparison.png) - ONLY if multiple datasets\n",
    "   - 2x2 grid of subplots:\n",
    "     - Top-left: Bar chart of average mean by dataset\n",
    "     - Top-right: Bar chart of average std by dataset\n",
    "     - Bottom-left: Bar chart of average range by dataset\n",
    "     - Bottom-right: Box plots of mean distribution by dataset\n",
    "''' if len(stats_paths) > 1 else ''}\n",
    "\n",
    "{'3' if len(stats_paths) > 1 else '2'}. **Distribution Analysis** (distributions.png)\n",
    "   - 2x2 grid of histograms:\n",
    "     - Mean distribution (with average line)\n",
    "     - Std distribution (with average line)\n",
    "     - Skewness distribution (with average line)\n",
    "     - Kurtosis distribution (with average line and threshold at 47)\n",
    "\n",
    "{'4' if len(stats_paths) > 1 else '3'}. **Box Plot Analysis** (boxplots.png)\n",
    "   - 1x3 grid:\n",
    "     - Min/Max/Range box plots\n",
    "     - Mean/Median box plots\n",
    "     - Std/Skewness/Kurtosis box plots (scaled for visibility)\n",
    "\n",
    "{'5' if len(stats_paths) > 1 else '4'}. **PCA Scatter Plot** (pca_scatter.png)\n",
    "   - Scatter plot: PC1 vs PC2\n",
    "   - Color by file index (temporal sequence) using 'viridis' colormap\n",
    "   - Annotate top 5% outliers (by distance from center)\n",
    "   - Add colorbar labeled 'File Index (Time)'\n",
    "\n",
    "{'6' if len(stats_paths) > 1 else '5'}. **Correlation Heatmap** (correlation_heatmap.png)\n",
    "   - Heatmap of correlations between all numeric metrics\n",
    "   - Use 'coolwarm' colormap, center at 0\n",
    "   - Annotate with correlation coefficients\n",
    "\n",
    "{'7' if len(stats_paths) > 1 else '6'}. **Control Chart** (control_chart.png)\n",
    "   - Line chart of mean values over file index\n",
    "   - Add center line (overall mean)\n",
    "   - Add ±3σ control limits (UCL/LCL)\n",
    "   - Shade ±2σ warning zone (yellow)\n",
    "   - Mark out-of-control points with red 'X'\n",
    "\n",
    "{'8' if len(stats_paths) > 1 else '7'}. **Radar Chart** (radar_chart.png)\n",
    "   - Compare top 5 vs bottom 5 performers\n",
    "   - Metrics: mean, std, range, skewness, kurtosis (normalized 0-1)\n",
    "   - Use polar plot with filled areas\n",
    "\n",
    "{'9' if len(stats_paths) > 1 else '8'}. **Summary Statistics Table** (summary_table.png)\n",
    "   - Create table image using matplotlib\n",
    "   - Show describe() output for all numeric columns\n",
    "   - Color-code header row (blue) and row labels (gray)\n",
    "\n",
    "**Technical Requirements:**\n",
    "- Save all charts to 'temp_charts/' directory (create if needed)\n",
    "- Use 300 DPI for all images\n",
    "- Professional color scheme: Blue (#1f77b4), Orange (#ff7f0e), Green (#2ca02c), Red (#d62728), Gray (#7f7f7f)\n",
    "- Set style: seaborn whitegrid\n",
    "- Add proper titles, labels, legends\n",
    "- Use tight_layout() before saving\n",
    "\n",
    "**Packages:**\n",
    "- matplotlib.pyplot\n",
    "- seaborn\n",
    "- pandas\n",
    "- numpy\n",
    "- sklearn.preprocessing.MinMaxScaler (for radar chart normalization)\n",
    "\n",
    "Print confirmation after each chart is saved.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE 2: VISUALIZATION GENERATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nGenerating {8 if len(stats_paths) == 1 else 9} professional visualizations...\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "viz_result, _ = client.chat_continue(\n",
    "    MODEL,\n",
    "    session_id,\n",
    "    visualization_prompt,\n",
    "    agent_type=\"auto\"\n",
    ")\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\nVisualization generation completed in {elapsed:.1f}s\\n\")\n",
    "print(\"=\" * 80)\n",
    "print(\"AI VISUALIZATION RESULT\")\n",
    "print(\"=\" * 80)\n",
    "print(viz_result)\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Phase 3: PowerPoint Assembly (AI-Driven)\n",
    "\n",
    "Let the AI agent create the PowerPoint presentation with all charts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get total file count for the prompt\n",
    "total_files = 0\n",
    "for path in stats_paths:\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        total_files += len(data.get('files', []))\n",
    "\n",
    "pptx_prompt = f\"\"\"\n",
    "Create a professional PowerPoint presentation using python-pptx.\n",
    "\n",
    "TASK: Generate Comprehensive PowerPoint Report\n",
    "\n",
    "**Presentation Details:**\n",
    "- Title: \"Warpage Analysis Report\"\n",
    "- Subtitle: \"Statistical Analysis of {total_files} Measurement Files{f' ({len(stats_paths)} Datasets)' if len(stats_paths) > 1 else ''}\"\n",
    "- Slide size: 10 x 7.5 inches\n",
    "- Output filename: Warpage_Report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pptx\n",
    "\n",
    "**Slide Structure:**\n",
    "\n",
    "**Slide 1: Title Slide**\n",
    "- Title: \"Warpage Analysis Report\" (44pt, bold, blue #1f77b4, centered)\n",
    "- Subtitle: \"Statistical Analysis of {total_files} Measurement Files{f' ({len(stats_paths)} Datasets)' if len(stats_paths) > 1 else ''}\" (24pt, centered)\n",
    "- Date: Current date (14pt, italic, centered)\n",
    "\n",
    "**Slide 2: Executive Summary**\n",
    "- Title: \"Executive Summary\" (32pt, bold, blue)\n",
    "- 4 metric cards (colored boxes with white text):\n",
    "  1. Total Files: {total_files} (blue background)\n",
    "  2. Avg Mean Warpage: [calculate from data] (orange background)\n",
    "  3. Avg Std Dev: [calculate from data] (green background)\n",
    "  4. Outliers (Kurtosis>47): [count from data] (red background)\n",
    "- Below cards: Add text summary from Phase 1 analysis\n",
    "\n",
    "**Slide 3: Temporal Trends**\n",
    "- Title: \"Temporal Trends\"\n",
    "- Image: temp_charts/temporal_trends.png\n",
    "- Description: \"Mean warpage values over time with ±1σ variability envelope\"\n",
    "\n",
    "{f'''**Slide 4: Dataset Comparison**\n",
    "- Title: \"Dataset Comparison\"\n",
    "- Image: temp_charts/dataset_comparison.png\n",
    "- Description: \"Comparative analysis across {len(stats_paths)} datasets\"\n",
    "''' if len(stats_paths) > 1 else ''}\n",
    "\n",
    "**Slide {'5' if len(stats_paths) > 1 else '4'}: Distribution Analysis**\n",
    "- Title: \"Distribution Analysis\"\n",
    "- Image: temp_charts/distributions.png\n",
    "- Description: \"Distribution of key statistical metrics\"\n",
    "\n",
    "**Slide {'6' if len(stats_paths) > 1 else '5'}: Variability Analysis**\n",
    "- Title: \"Variability Analysis\"\n",
    "- Image: temp_charts/boxplots.png\n",
    "- Description: \"Box plots showing spread and quartiles\"\n",
    "\n",
    "**Slide {'7' if len(stats_paths) > 1 else '6'}: PCA Analysis**\n",
    "- Title: \"PCA Analysis\"\n",
    "- Image: temp_charts/pca_scatter.png\n",
    "- Description: \"Principal Component Analysis with outlier detection\"\n",
    "\n",
    "**Slide {'8' if len(stats_paths) > 1 else '7'}: Correlation Matrix**\n",
    "- Title: \"Correlation Matrix\"\n",
    "- Image: temp_charts/correlation_heatmap.png\n",
    "- Description: \"Correlations between all metrics\"\n",
    "\n",
    "**Slide {'9' if len(stats_paths) > 1 else '8'}: Control Chart**\n",
    "- Title: \"Outlier Detection (Control Chart)\"\n",
    "- Image: temp_charts/control_chart.png\n",
    "- Description: \"Statistical process control with ±3σ limits\"\n",
    "\n",
    "**Slide {'10' if len(stats_paths) > 1 else '9'}: Top vs Bottom Performers**\n",
    "- Title: \"Top vs Bottom Performers\"\n",
    "- Image: temp_charts/radar_chart.png\n",
    "- Description: \"Comparison of best vs worst quality measurements\"\n",
    "\n",
    "**Slide {'11' if len(stats_paths) > 1 else '10'}: Summary Statistics**\n",
    "- Title: \"Summary Statistics\"\n",
    "- Image: temp_charts/summary_table.png\n",
    "- Description: \"Descriptive statistics for all metrics\"\n",
    "\n",
    "**Slide {'12' if len(stats_paths) > 1 else '11'}: Recommendations**\n",
    "- Title: \"Recommendations & Next Steps\" (32pt, bold, blue)\n",
    "- Three sections with bullet points:\n",
    "  1. Quality Control (orange header, 18pt bold)\n",
    "     - Investigate outlier files\n",
    "     - Review out-of-control points\n",
    "     - Establish tighter control limits\n",
    "  2. Process Improvement (orange header, 18pt bold)\n",
    "     - Target: mean warpage closer to 0\n",
    "     - Reduce variability\n",
    "     - Implement corrective actions\n",
    "  3. Monitoring (orange header, 18pt bold)\n",
    "     - Track PCA trends\n",
    "     - Set up automated alerts\n",
    "     - Conduct root cause analysis\n",
    "\n",
    "**Technical Requirements:**\n",
    "- Use blank layout (index 6) for all slides\n",
    "- Add textboxes with specified formatting\n",
    "- For chart slides:\n",
    "  - Title at top (0.5 inches from top)\n",
    "  - Image centered (9 inches wide)\n",
    "  - Description at bottom if needed\n",
    "- RGB colors: Blue (31, 119, 180), Orange (255, 127, 14), Green (44, 160, 44), Red (214, 39, 40)\n",
    "- Use PP_ALIGN.CENTER for titles\n",
    "- Save with timestamp in filename\n",
    "\n",
    "Print confirmation when PowerPoint is saved, including:\n",
    "- Filename\n",
    "- File size\n",
    "- Number of slides\n",
    "- Full file path\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE 3: POWERPOINT ASSEMBLY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nCreating PowerPoint with {11 if len(stats_paths) == 1 else 12} slides...\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "pptx_result, _ = client.chat_continue(\n",
    "    MODEL,\n",
    "    session_id,\n",
    "    pptx_prompt,\n",
    "    agent_type=\"auto\"\n",
    ")\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\nPowerPoint creation completed in {elapsed:.1f}s\\n\")\n",
    "print(\"=\" * 80)\n",
    "print(\"AI POWERPOINT RESULT\")\n",
    "print(\"=\" * 80)\n",
    "print(pptx_result)\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"REPORT GENERATION COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Find generated PowerPoint\n",
    "pptx_files = sorted(glob.glob(\"Warpage_Report_*.pptx\"), reverse=True)\n",
    "if pptx_files:\n",
    "    latest_pptx = pptx_files[0]\n",
    "    size_kb = Path(latest_pptx).stat().st_size / 1024\n",
    "    print(f\"\\nGenerated PowerPoint:\")\n",
    "    print(f\"  File: {latest_pptx}\")\n",
    "    print(f\"  Size: {size_kb:.2f} KB\")\n",
    "else:\n",
    "    print(\"\\nWarning: Could not find generated PowerPoint file\")\n",
    "\n",
    "# Check generated charts\n",
    "temp_charts = Path(\"temp_charts\")\n",
    "if temp_charts.exists():\n",
    "    chart_files = list(temp_charts.glob(\"*.png\"))\n",
    "    print(f\"\\nGenerated Charts: {len(chart_files)}\")\n",
    "    for chart in sorted(chart_files):\n",
    "        print(f\"  - {chart.name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"WHAT THE AI DID\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "Phase 1: Data Analysis\n",
    "  - Loaded and combined all datasets\n",
    "  - Calculated key statistics\n",
    "  - Identified patterns and outliers\n",
    "  - Recommended visualizations\n",
    "\n",
    "Phase 2: Visualization Generation\n",
    "  - Generated 8-9 professional charts\n",
    "  - Used matplotlib/seaborn with 300 DPI\n",
    "  - Applied professional color schemes\n",
    "  - Saved all charts to temp_charts/\n",
    "\n",
    "Phase 3: PowerPoint Assembly\n",
    "  - Created 11-12 slide presentation\n",
    "  - Added title, summary, charts, and recommendations\n",
    "  - Formatted with professional styling\n",
    "  - Saved with timestamp\n",
    "\n",
    "Total Steps: All autonomous via ReAct agent + python_coder tool!\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"NEXT STEPS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "1. Open the generated .pptx file\n",
    "2. Review charts and insights\n",
    "3. Customize branding/colors if needed\n",
    "4. Add company logo\n",
    "5. Present to stakeholders\n",
    "\n",
    "To regenerate with different data:\n",
    "  - Update stats_paths in Section 2\n",
    "  - Run all cells again\n",
    "\n",
    "To cleanup temporary files:\n",
    "  - Delete temp_charts/ directory\n",
    "  - Delete old .pptx files\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Optional: Cleanup\n",
    "\n",
    "Remove temporary chart files if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "cleanup = input(\"Delete temporary chart files? (y/n): \")\n",
    "if cleanup.lower() == 'y':\n",
    "    temp_charts = Path(\"temp_charts\")\n",
    "    if temp_charts.exists():\n",
    "        shutil.rmtree(temp_charts)\n",
    "        print(f\"✓ Cleaned up {temp_charts}\")\n",
    "else:\n",
    "    print(f\"Temporary charts preserved in: temp_charts/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## About This Notebook\n",
    "\n",
    "This notebook demonstrates the power of **AI-orchestrated automation**:\n",
    "\n",
    "- **~200 lines of orchestration code** vs 2000+ lines of hardcoded logic\n",
    "- **Adaptive**: AI adjusts to your data structure\n",
    "- **Self-healing**: Auto-fixes code errors (max 5 retries)\n",
    "- **Multi-file support**: Built-in from your API\n",
    "- **Leverages your infrastructure**: ReAct agent + python_coder tool\n",
    "\n",
    "**How it works:**\n",
    "1. You provide strategic prompts\n",
    "2. ReAct agent reasons about the task\n",
    "3. python_coder generates code\n",
    "4. Code executes in sandboxed environment\n",
    "5. Agent verifies results and iterates if needed\n",
    "\n",
    "**Result:** Comprehensive PowerPoint report generated autonomously!\n",
    "\n",
    "---\n",
    "\n",
    "**Version:** 1.0.0 (AI-Orchestrated)  \n",
    "**Last Updated:** January 2025"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
