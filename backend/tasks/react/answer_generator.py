"""
ReAct Answer Generator

Handles final answer generation for the ReAct agent by synthesizing observations
from all execution steps into a coherent final answer.

Uses two complementary strategies:
1. LLM-based generation: Synthesizes comprehensive answer
2. Fallback extraction: Extracts answer directly from observations when LLM fails
"""

from typing import List
from langchain_core.messages import HumanMessage

from .models import ToolName, ReActStep
from backend.config import prompts
from backend.utils.logging_utils import get_logger

logger = get_logger(__name__)


class AnswerGenerator:
    """
    Handles final answer generation for ReAct agent execution.

    The AnswerGenerator synthesizes all accumulated ReAct steps into a coherent
    final answer using LLM generation with fallback extraction.

    Design Principles:
    - No information loss: Ensures observations from all steps are considered
    - Robust fallbacks: Multiple strategies to handle edge cases
    - Context-aware: Uses intelligent context formatting
    """

    def __init__(self, llm):
        """
        Initialize the AnswerGenerator.

        Args:
            llm: LangChain LLM instance (e.g., ChatOllama)
        """
        self.llm = llm

    async def generate_final_answer(self, user_query: str, steps: List[ReActStep]) -> str:
        """
        Generate final answer by synthesizing all observations using the LLM.

        This is the primary answer generation strategy. Formats all ReAct steps
        into context and prompts the LLM to synthesize a comprehensive answer.

        Context formatting:
        - For ≤3 steps: Includes all steps with full detail
        - For >3 steps: Summarizes early steps, shows last 2 in detail

        Args:
            user_query: The original user question/request
            steps: List of ReActStep objects representing execution history

        Returns:
            Final answer string generated by the LLM. May be empty if LLM call fails.
        """
        context = self._format_steps_context(steps)

        # Use centralized prompt
        prompt = prompts.get_react_final_answer_prompt(query=user_query, context=context)

        logger.info("Final answer generation requested")

        response = await self.llm.ainvoke([HumanMessage(content=prompt)])

        logger.info("LLM OUTPUT (Final Answer Generation):")
        logger.info(response.content.strip()[:500] + "...")

        return response.content.strip()

    def extract_from_steps(self, user_query: str, steps: List[ReActStep]) -> str:
        """
        Extract answer directly from observation history (fallback strategy).

        Used when LLM-based answer generation fails or produces insufficient answer.

        Multi-tier fallback strategy:
        1. Recent observation extraction: Scans steps in reverse for informative observations
        2. Attempted tools summary: Returns summary of what tools were tried
        3. Generic fallback: Returns apologetic message

        Args:
            user_query: The original user question/request
            steps: List of ReActStep objects

        Returns:
            Extracted answer string. Never returns empty - always provides some response.
        """
        if not steps:
            return "I apologize, but I was unable to generate a proper response. Please try rephrasing your question."

        # Try to find the most informative observation from recent steps
        for step in reversed(steps):
            obs = step.observation.strip()
            # Skip error messages and empty observations
            if obs and len(obs) >= 20 and not obs.startswith("Error") and not obs.startswith("No "):
                logger.info(f"[AnswerGenerator] Extracted answer from step {step.step_num} observation")
                return f"Based on my research: {obs}"

        # If no good observation found, summarize what was attempted
        actions_taken = [step.action for step in steps if step.action != ToolName.FINISH]
        if actions_taken:
            return f"I attempted to answer your question using {', '.join(set(actions_taken))}, but was unable to find sufficient information. Please try rephrasing your question or providing more context."

        return "I apologize, but I was unable to generate a proper response. Please try rephrasing your question."

    def _format_steps_context(self, steps: List[ReActStep]) -> str:
        """
        Format previous steps with intelligent context pruning.

        This method implements performance optimization that reduces token usage
        and improves LLM processing speed:

        - For ≤3 steps: Returns all steps with full detail
        - For >3 steps: Returns summary of early steps + last 2 steps in detail

        Context pruning strategy:
        - Early steps: Summarized as "Steps 1-N completed using: <tools>"
        - Recent steps: Full detail including thought, action, input, observation

        Args:
            steps: List of ReActStep objects to format

        Returns:
            Formatted context string ready for LLM prompt. Empty string if no steps.
        """
        if not steps:
            return ""

        # If few steps, return all details
        if len(steps) <= 3:
            return self._format_all_steps(steps)

        # Context pruning: summary + recent steps
        context_parts = ["Previous Steps:\n"]

        # Summary of early steps
        early_steps = steps[:-2]
        tools_used = list(set([s.action for s in early_steps if s.action != ToolName.FINISH]))
        summary = f"Steps 1-{len(early_steps)} completed using: {', '.join(tools_used)}"
        context_parts.append(f"[Summary] {summary}\n")

        # Recent steps in full detail
        context_parts.append("\n[Recent Steps - Full Detail]")
        recent_steps = steps[-2:]
        for step in recent_steps:
            obs_display = step.observation[:500] if len(step.observation) > 500 else step.observation
            context_parts.append(f"""
Step {step.step_num}:
- Thought: {step.thought[:200]}...
- Action: {step.action}
- Action Input: {step.action_input[:200] if len(step.action_input) > 200 else step.action_input}
- Observation: {obs_display}
""")

        return "\n".join(context_parts)

    def _format_all_steps(self, steps: List[ReActStep]) -> str:
        """
        Format all steps in full detail (used when step count is low).

        This method is used when the number of steps is small (≤3) and we can
        afford to include all steps without context pruning.

        Args:
            steps: List of ReActStep objects to format

        Returns:
            Formatted string containing all steps with full detail
        """
        context_parts = ["Previous Steps:"]
        for step in steps:
            context_parts.append(f"""
Step {step.step_num}:
- Thought: {step.thought}
- Action: {step.action}
- Action Input: {step.action_input}
- Observation: {step.observation}
""")
        return "\n".join(context_parts)
