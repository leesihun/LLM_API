"""
ReAct Answer Generator

This module handles the final answer generation phase of the ReAct agent.
It synthesizes observations from all execution steps into a coherent final answer
using two complementary strategies:

1. LLM-based generation: Uses the language model to synthesize a comprehensive answer
2. Fallback extraction: Extracts answer directly from observations when LLM generation fails

The AnswerGenerator is designed to ensure that no valuable information collected
during the ReAct execution loop is lost in the final answer.
"""

from typing import List
from langchain_core.messages import HumanMessage

from .models import ToolName, ReActStep
from backend.config import prompts
from backend.utils.logging_utils import get_logger

logger = get_logger(__name__)


class AnswerGenerator:
    """
    Handles final answer generation for ReAct agent execution.

    The AnswerGenerator takes all accumulated ReAct steps (Thought-Action-Observation cycles)
    and synthesizes them into a coherent final answer. It uses two complementary strategies:

    1. **LLM Generation** (Primary): Calls the language model with a specialized prompt
       that includes all execution context to generate a comprehensive answer.

    2. **Fallback Extraction** (Secondary): If LLM generation fails or produces an
       insufficient answer, directly extracts relevant information from observations.

    Design Principles:
    - **No information loss**: Ensures observations from all steps are considered
    - **Robust fallbacks**: Multiple strategies to handle edge cases
    - **Context-aware**: Uses intelligent context formatting for better LLM performance

    Attributes:
        llm: Language model instance for answer generation

    Example:
        ```python
        generator = AnswerGenerator(llm)

        # After ReAct execution completes with steps
        final_answer = await generator.generate_final_answer(
            user_query="What is the capital of France?",
            steps=react_steps
        )

        # If LLM generation fails, use fallback
        if not final_answer or len(final_answer.strip()) < 10:
            final_answer = generator.extract_from_steps(
                user_query="What is the capital of France?",
                steps=react_steps
            )
        ```
    """

    def __init__(self, llm):
        """
        Initialize the AnswerGenerator.

        Args:
            llm: LangChain LLM instance (e.g., ChatOllama) used for final answer generation.
                 Must support async ainvoke() method.
        """
        self.llm = llm

    async def generate_final_answer(self, user_query: str, steps: List[ReActStep]) -> str:
        """
        Generate final answer by synthesizing all observations using the LLM.

        This is the primary answer generation strategy. It formats all ReAct steps
        into a context string and prompts the LLM to synthesize a comprehensive
        final answer that addresses the user's original query.

        The method uses intelligent context formatting:
        - For ≤3 steps: Includes all steps with full detail
        - For >3 steps: Summarizes early steps, shows last 2 in detail (context pruning)

        Args:
            user_query: The original user question/request
            steps: List of ReActStep objects representing the execution history.
                   Each step contains thought, action, action_input, and observation.

        Returns:
            Final answer string generated by the LLM. May be empty if LLM call fails.

        Raises:
            Exception: If LLM invocation fails (caller should handle and use fallback)

        Example:
            ```python
            steps = [
                ReActStep(1),  # with web_search observation
                ReActStep(2),  # with python_coder observation
            ]
            answer = await generator.generate_final_answer(
                "Analyze sales.csv",
                steps
            )
            # Returns: "Based on the analysis of sales.csv, total revenue is..."
            ```

        Note:
            - Relies on prompts.get_react_final_answer_prompt() for prompt template
            - Uses _format_steps_context() for intelligent context formatting
            - Logs generation request and LLM output for debugging
        """
        context = self._format_steps_context(steps)

        # Use centralized prompt
        prompt = prompts.get_react_final_answer_prompt(query=user_query, context=context)

        logger.info("")
        logger.info("Final answer generation requested")
        logger.info("")

        response = await self.llm.ainvoke([HumanMessage(content=prompt)])

        logger.info("")
        logger.info("LLM OUTPUT (Final Answer Generation):")
        for _line in response.content.strip().splitlines():
            logger.info(_line)
        logger.info("")

        return response.content.strip()

    def extract_from_steps(self, user_query: str, steps: List[ReActStep]) -> str:
        """
        Extract answer directly from observation history (fallback strategy).

        This method is used when LLM-based answer generation fails or produces
        an insufficient answer. It implements a multi-tier fallback strategy:

        1. **Recent observation extraction**: Scans steps in reverse order looking
           for informative observations (≥20 chars, not error messages)

        2. **Attempted tools summary**: If no good observation found, returns a
           summary of what tools were tried

        3. **Generic fallback**: Returns apologetic message as last resort

        Args:
            user_query: The original user question/request (currently unused but
                       included for API consistency and future enhancements)
            steps: List of ReActStep objects representing the execution history

        Returns:
            Extracted answer string. Never returns empty string - always provides
            some response even if it's a generic fallback message.

        Example:
            ```python
            # When LLM generation fails
            fallback_answer = generator.extract_from_steps(
                "What is the weather?",
                steps
            )
            # Returns: "Based on my research: Current weather is 72°F and sunny"
            ```

        Fallback Behavior:
            - No steps: Returns generic apology
            - No valid observations: Returns summary of attempted tools
            - Valid observation found: Returns "Based on my research: <observation>"

        Note:
            This is a synchronous method (no LLM calls), making it fast and reliable
            for emergency fallback scenarios.
        """
        if not steps:
            return "I apologize, but I was unable to generate a proper response. Please try rephrasing your question."

        # Try to find the most informative observation from recent steps
        for step in reversed(steps):
            obs = step.observation.strip()
            # Skip error messages and empty observations
            if obs and len(obs) >= 20 and not obs.startswith("Error") and not obs.startswith("No "):
                logger.info(f"[AnswerGenerator] Extracted answer from step {step.step_num} observation")
                return f"Based on my research: {obs}"

        # If no good observation found, summarize what was attempted
        actions_taken = [step.action for step in steps if step.action != ToolName.FINISH]
        if actions_taken:
            return f"I attempted to answer your question using {', '.join(set(actions_taken))}, but was unable to find sufficient information. Please try rephrasing your question or providing more context."

        return "I apologize, but I was unable to generate a proper response. Please try rephrasing your question."

    def _format_steps_context(self, steps: List[ReActStep]) -> str:
        """
        Format previous steps with intelligent context pruning for LLM consumption.

        This method implements a performance optimization that reduces token usage
        and improves LLM processing speed by intelligently formatting step history:

        - **For ≤3 steps**: Returns all steps with full detail
        - **For >3 steps**: Returns summary of early steps + last 2 steps in detail

        Context pruning strategy:
        - Early steps (all except last 2): Summarized as "Steps 1-N completed using: <tools>"
        - Recent steps (last 2): Full detail including thought, action, input, observation

        This approach balances two goals:
        1. Provide enough context for informed answer generation
        2. Minimize token usage and processing time

        Args:
            steps: List of ReActStep objects to format

        Returns:
            Formatted context string ready for inclusion in LLM prompt.
            Returns empty string if no steps provided.

        Example Output (>3 steps):
            ```
            Previous Steps:

            [Summary] Steps 1-3 completed using: web_search, python_coder

            [Recent Steps - Full Detail]
            Step 4:
            - Thought: Need to calculate average
            - Action: python_coder
            - Action Input: Calculate average of sales
            - Observation: Average sales: $45,230

            Step 5:
            - Thought: Results ready to present
            - Action: finish
            - Action Input: Present final analysis
            - Observation: Task completed
            ```

        Performance Impact:
            For a 10-step execution, this reduces context size by ~60% while
            maintaining quality of final answer generation.
        """
        if not steps:
            return ""

        # If few steps, return all details
        if len(steps) <= 3:
            return self._format_all_steps(steps)

        # Context pruning: summary + recent steps
        context_parts = ["Previous Steps:\n"]

        # Summary of early steps
        early_steps = steps[:-2]
        tools_used = list(set([s.action for s in early_steps if s.action != ToolName.FINISH]))
        summary = f"Steps 1-{len(early_steps)} completed using: {', '.join(tools_used)}"
        context_parts.append(f"[Summary] {summary}\n")

        # Recent steps in full detail
        context_parts.append("\n[Recent Steps - Full Detail]")
        recent_steps = steps[-2:]
        for step in recent_steps:
            obs_display = step.observation[:500] if len(step.observation) > 500 else step.observation
            context_parts.append(f"""
Step {step.step_num}:
- Thought: {step.thought[:200]}...
- Action: {step.action}
- Action Input: {step.action_input[:200] if len(step.action_input) > 200 else step.action_input}
- Observation: {obs_display}
""")

        return "\n".join(context_parts)

    def _format_all_steps(self, steps: List[ReActStep]) -> str:
        """
        Format all steps in full detail (used when step count is low).

        This method is used when the number of steps is small (≤3) and we can
        afford to include all steps without context pruning. It provides complete
        detail for every step, ensuring no information is lost.

        Args:
            steps: List of ReActStep objects to format

        Returns:
            Formatted string containing all steps with full detail

        Example Output:
            ```
            Previous Steps:
            Step 1:
            - Thought: Need to search for recent data
            - Action: web_search
            - Action Input: latest AI developments
            - Observation: Found 5 articles discussing GPT-4 and...

            Step 2:
            - Thought: Need to analyze the findings
            - Action: python_coder
            - Action Input: Summarize key themes
            - Observation: Key themes include: multimodal AI, efficiency...
            ```

        Note:
            Unlike _format_steps_context(), this method does NOT truncate
            observations, thoughts, or action inputs. All content is preserved.
        """
        context_parts = ["Previous Steps:"]
        for step in steps:
            obs_display = step.observation[:]
            context_parts.append(f"""
Step {step.step_num}:
- Thought: {step.thought}
- Action: {step.action}
- Action Input: {step.action_input}
- Observation: {obs_display}
""")
        return "\n".join(context_parts)
