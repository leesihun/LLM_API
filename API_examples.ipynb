{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM API – End-to-End Examples (Single Notebook)\n",
    "\n",
    "This notebook shows a minimal client and step-by-step examples for:\n",
    "\n",
    "1. Create a new account\n",
    "2. Login\n",
    "3. Change models (admin privilege example)\n",
    "4. Start a new chat and get a response\n",
    "5. Continue a chat\n",
    "6. See chat history\n",
    "7. Websearch with agentic tool selection\n",
    "8. Agentic math calculation (LLM decides to use math tool)\n",
    "9. Sequential reasoning with ReAct agent (step-by-step thinking)\n",
    "10. Plan-and-Execute agent (parallel tool usage)\n",
    "11. Auto agent selection (smart router picks best agent)\n",
    "12. Complex JSON data analysis\n",
    "13. Real Data Analysis - Warpage Statistics (using 20251013_stats.json)\n",
    "14. Python Code Generation - Simple Calculation\n",
    "15. Python Code Generation - Data Analysis\n",
    "16. Python Code Generation - Mathematical Computation\n",
    "17. Python Code Generation - String Processing\n",
    "18. Python Code Generation - Excel File Analysis (Real File)\n",
    "\n",
    "Set your API base URL below if different from the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install httpx\n",
    "! pip install pip-system-certs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_BASE_URL = \"http://10.252.38.241:1007\"\n",
    "print(\"Using:\", API_BASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "class LLMApiClient:\n",
    "    def __init__(self, base_url: str, timeout: float = 1200.0):\n",
    "        \"\"\"\n",
    "        Initialize the LLM API client.\n",
    "\n",
    "        Args:\n",
    "            base_url: API base URL\n",
    "            timeout: Request timeout in seconds (default: 1200s/20min for LLM requests)\n",
    "        \"\"\"\n",
    "        self.base_url = base_url.rstrip(\"/\")\n",
    "        self.token = None\n",
    "        # Create timeout config: 10s for connect, custom timeout for read/write/pool\n",
    "        self.timeout = httpx.Timeout(50.0, read=timeout, write=timeout, pool=timeout)\n",
    "\n",
    "    def _headers(self):\n",
    "        # Don't set Content-Type - httpx auto-sets for multipart\n",
    "        h = {}\n",
    "        if self.token:\n",
    "            h[\"Authorization\"] = f\"Bearer {self.token}\"\n",
    "        return h\n",
    "\n",
    "    def signup(self, username: str, password: str, role: str = \"guest\"):\n",
    "        r = httpx.post(f\"{self.base_url}/api/auth/signup\", json={\n",
    "            \"username\": username, \"password\": password, \"role\": role\n",
    "        }, timeout=10.0)\n",
    "        r.raise_for_status()\n",
    "        return r.json()\n",
    "\n",
    "    def login(self, username: str, password: str):\n",
    "        r = httpx.post(f\"{self.base_url}/api/auth/login\", json={\n",
    "            \"username\": username, \"password\": password\n",
    "        }, timeout=10.0)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        self.token = data[\"access_token\"]\n",
    "        return data\n",
    "\n",
    "    def list_models(self):\n",
    "        # JSON endpoints still use Content-Type header\n",
    "        headers = {\"Authorization\": f\"Bearer {self.token}\"} if self.token else {}\n",
    "        r = httpx.get(f\"{self.base_url}/v1/models\", headers=headers, timeout=10.0)\n",
    "        r.raise_for_status()\n",
    "        return r.json()\n",
    "\n",
    "    def change_model(self, model: str):\n",
    "        headers = {\"Authorization\": f\"Bearer {self.token}\", \"Content-Type\": \"application/json\"} if self.token else {\"Content-Type\": \"application/json\"}\n",
    "        r = httpx.post(f\"{self.base_url}/api/admin/model\", json={\"model\": model}, headers=headers, timeout=10.0)\n",
    "        r.raise_for_status()\n",
    "        return r.json()\n",
    "\n",
    "    def chat_new(self, model: str, user_message: str, agent_type: str = \"auto\", files: list = None):\n",
    "        \"\"\"\n",
    "        Start new chat with optional file attachments (multipart/form-data)\n",
    "        \n",
    "        Args:\n",
    "            model: Model name\n",
    "            user_message: User message\n",
    "            agent_type: Agent type (auto, react, plan_execute)\n",
    "            files: Optional list of file paths to attach\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (response_text, session_id)\n",
    "        \"\"\"\n",
    "        messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "        \n",
    "        # Prepare form data\n",
    "        data = {\n",
    "            \"model\": model,\n",
    "            \"messages\": json.dumps(messages),\n",
    "            \"agent_type\": agent_type\n",
    "        }\n",
    "        \n",
    "        # Prepare files for upload\n",
    "        files_to_upload = []\n",
    "        if files:\n",
    "            for file_path in files:\n",
    "                f = open(file_path, \"rb\")\n",
    "                files_to_upload.append((\"files\", (Path(file_path).name, f)))\n",
    "        \n",
    "        try:\n",
    "            r = httpx.post(\n",
    "                f\"{self.base_url}/v1/chat/completions\",\n",
    "                data=data,\n",
    "                files=files_to_upload if files_to_upload else None,\n",
    "                headers=self._headers(),\n",
    "                timeout=self.timeout\n",
    "            )\n",
    "            r.raise_for_status()\n",
    "            result = r.json()\n",
    "            return result[\"choices\"][0][\"message\"][\"content\"], result[\"x_session_id\"]\n",
    "        \n",
    "        finally:\n",
    "            # Close file handles\n",
    "            for _, (_, f) in files_to_upload:\n",
    "                f.close()\n",
    "\n",
    "    def chat_continue(self, model: str, session_id: str, user_message: str, agent_type: str = \"auto\", files: list = None):\n",
    "        \"\"\"\n",
    "        Continue existing chat with optional file attachments (multipart/form-data)\n",
    "        \n",
    "        Args:\n",
    "            model: Model name\n",
    "            session_id: Session ID to continue\n",
    "            user_message: User message\n",
    "            agent_type: Agent type (auto, react, plan_execute)\n",
    "            files: Optional list of file paths to attach\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (response_text, session_id)\n",
    "        \"\"\"\n",
    "        messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "        \n",
    "        data = {\n",
    "            \"model\": model,\n",
    "            \"messages\": json.dumps(messages),\n",
    "            \"session_id\": session_id,\n",
    "            \"agent_type\": agent_type\n",
    "        }\n",
    "        \n",
    "        files_to_upload = []\n",
    "        if files:\n",
    "            for file_path in files:\n",
    "                f = open(file_path, \"rb\")\n",
    "                files_to_upload.append((\"files\", (Path(file_path).name, f)))\n",
    "        \n",
    "        try:\n",
    "            r = httpx.post(\n",
    "                f\"{self.base_url}/v1/chat/completions\",\n",
    "                data=data,\n",
    "                files=files_to_upload if files_to_upload else None,\n",
    "                headers=self._headers(),\n",
    "                timeout=self.timeout\n",
    "            )\n",
    "            r.raise_for_status()\n",
    "            result = r.json()\n",
    "            return result[\"choices\"][0][\"message\"][\"content\"], result[\"x_session_id\"]\n",
    "        \n",
    "        finally:\n",
    "            for _, (_, f) in files_to_upload:\n",
    "                f.close()\n",
    "\n",
    "    def chat_sessions(self):\n",
    "        r = httpx.get(f\"{self.base_url}/api/chat/sessions\", headers=self._headers(), timeout=10.0)\n",
    "        r.raise_for_status()\n",
    "        return r.json()[\"sessions\"]\n",
    "\n",
    "    def chat_history(self, session_id: str):\n",
    "        r = httpx.get(f\"{self.base_url}/api/chat/history/{session_id}\", headers=self._headers(), timeout=10.0)\n",
    "        r.raise_for_status()\n",
    "        return r.json()[\"messages\"]\n",
    "\n",
    "    def tools(self):\n",
    "        r = httpx.get(f\"{self.base_url}/api/tools/list\", headers=self._headers(), timeout=10.0)\n",
    "        r.raise_for_status()\n",
    "        return r.json()[\"tools\"]\n",
    "\n",
    "    def websearch(self, query: str, max_results: int = 5):\n",
    "        \"\"\"\n",
    "        Perform web search and get LLM-generated answer\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with:\n",
    "            - answer: LLM-generated answer from search results\n",
    "            - results: Raw search results (list of dicts with title, url, content, score)\n",
    "            - sources_used: List of URLs used as sources\n",
    "        \"\"\"\n",
    "        headers = {\"Authorization\": f\"Bearer {self.token}\", \"Content-Type\": \"application/json\"} if self.token else {\"Content-Type\": \"application/json\"}\n",
    "        r = httpx.post(f\"{self.base_url}/api/tools/websearch\", json={\"query\": query, \"max_results\": max_results}, headers=headers, timeout=60.0)\n",
    "        r.raise_for_status()\n",
    "        return r.json()  # Returns full response with answer, results, and sources_used\n",
    "\n",
    "    def answer_from_json(self, model: str, json_blob: dict, question: str):\n",
    "        prompt = f\"Given this JSON: {json_blob}\\nAnswer: {question}\"\n",
    "        return self.chat_new(model, prompt)[0]\n",
    "\n",
    "client = LLMApiClient(API_BASE_URL, timeout=1200.0)  # 20 minute timeout\n",
    "print(\"Client ready with 1200s (20 min) timeout for chat requests\")\n",
    "print(\"✓ Now supports multipart/form-data with optional file attachments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Create a new account (skip if user already exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "username = \"leesihun\"\n",
    "password = \"s.hun.lee\"\n",
    "try:\n",
    "    result = client.signup(username, password)\n",
    "    print(f\"Account created: {result}\")\n",
    "except Exception as e:\n",
    "    print(f\"Signup skipped (user may already exist): {e}\")\n",
    "    print(\"Continuing with existing account...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "login = client.login(username, password)\n",
    "login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Change models (admin only) – optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Using deepseek-r1:1.5b for all examples\n",
    "client.login(\"admin\", \"administrator\")\n",
    "client.change_model(\"gpt-oss:20b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List models (OpenAI-compatible)\n",
    "models = client.list_models()\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Start a new chat and get a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODEL = models[\"data\"][0][\"id\"]\n",
    "reply, session_id = client.chat_new(MODEL, \"Hello! Give me a short haiku about autumn.\")\n",
    "reply, session_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Continue an existing chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reply2, _ = client.chat_continue(MODEL, session_id, \"Now do one about winter.\")\n",
    "reply2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) See chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client.chat_sessions(), client.chat_history(session_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7) Websearch with LLM-generated answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The API now generates an answer from search results using LLM\n",
    "client.login(\"leesihun\", \"s.hun.lee\")\n",
    "search_query = \"Tell me who is SiHun Lee\"\n",
    "search_response = client.websearch(search_query)\n",
    "\n",
    "print(\"=== LLM-Generated Answer ===\")\n",
    "print(search_response[\"answer\"])\n",
    "print(\"\\n=== Sources Used ===\")\n",
    "for i, url in enumerate(search_response[\"sources_used\"], 1):\n",
    "    print(f\"{i}. {url}\")\n",
    "print(\"\\n=== Raw Search Results (for reference) ===\")\n",
    "import pprint\n",
    "pprint.pprint(search_response[\"results\"][:2])  # Show first 2 raw results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7b) Websearch example - Sports news\n",
    "# Another example showing the LLM answer generation\n",
    "client.login(\"leesihun\", \"s.hun.lee\")\n",
    "search_query = \"What was the latest game of Liverpool FC and who won?\"\n",
    "search_response = client.websearch(search_query)\n",
    "\n",
    "print(\"=== LLM-Generated Answer ===\")\n",
    "print(search_response[\"answer\"])\n",
    "print(\"\\n=== Sources ===\")\n",
    "for i, url in enumerate(search_response[\"sources_used\"], 1):\n",
    "    print(f\"{i}. {url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8) Agentic tool usage - Let the LLM decide which tool to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Simple math question (agent will automatically use math_calculator tool)\n",
    "math_reply, _ = client.chat_new(MODEL, \"What is 11.951/3.751?\")\n",
    "print(\"Math Question Response:\")\n",
    "from IPython.display import display, Math, Latex\n",
    "display(Latex(math_reply))\n",
    "print(math_reply)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9) Sequential reasoning with ReAct agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This triggers the ReAct agent because it requires step-by-step thinking\n",
    "sequential_query = \"\"\"\n",
    "First, search the web to find the current population of Tokyo.\n",
    "Then, calculate what 15% of that population would be.\n",
    "Finally, tell me the result.\n",
    "Think hard, try to answer to best of your knowledge\n",
    "\"\"\"\n",
    "react_reply, _ = client.chat_new(MODEL, sequential_query, agent_type=\"react\")\n",
    "print(\"Sequential Reasoning (ReAct) Response:\")\n",
    "\n",
    "display(Latex(react_reply))\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10) Plan-and-Execute agent with multiple tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This triggers Plan-and-Execute agent because it uses \"and\" for parallel tasks\n",
    "parallel_query = \"\"\"\n",
    "Search for the latest news about artificial intelligence and\n",
    "calculate the result of (100 * 0.15 + 25) / 2 and\n",
    "Think about who is god and\n",
    "What is the best Korean food and\n",
    "what is 1007*1007/4524753.\n",
    "\"\"\"\n",
    "plan_reply, _ = client.chat_new(MODEL, parallel_query, agent_type=\"plan_execute\")\n",
    "print(\"Plan-and-Execute Response:\")\n",
    "\n",
    "display(Latex(plan_reply))\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11) Auto agent selection - Let the router decide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The smart router will analyze the query and pick the best agent\n",
    "auto_query = \"If the capital of France has a population of 2.1 million, and we need to allocate 500 euros per person for a project, what's the total budget needed? First search for the actual population, then calculate.\"\n",
    "auto_reply, _ = client.chat_new(MODEL, auto_query, agent_type=\"auto\")\n",
    "print(\"Auto Agent Selection Response:\")\n",
    "print(auto_reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12) Complex JSON data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12) Complex JSON data analysis (with File Upload)\n",
    "import json\n",
    "\n",
    "# Create a realistic e-commerce dataset\n",
    "complex_json = {\n",
    "    \"company\": \"TechMart Inc\",\n",
    "    \"quarter\": \"Q3 2025\",\n",
    "    \"departments\": [\n",
    "        {\n",
    "            \"name\": \"Electronics\",\n",
    "            \"employees\": 45,\n",
    "            \"sales\": [\n",
    "                {\"product\": \"Laptop\", \"units_sold\": 320, \"price\": 1200, \"revenue\": 384000},\n",
    "                {\"product\": \"Smartphone\", \"units_sold\": 856, \"price\": 800, \"revenue\": 684800},\n",
    "                {\"product\": \"Tablet\", \"units_sold\": 142, \"price\": 500, \"revenue\": 71000}\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Home Appliances\",\n",
    "            \"employees\": 32,\n",
    "            \"sales\": [\n",
    "                {\"product\": \"Refrigerator\", \"units_sold\": 89, \"price\": 1500, \"revenue\": 133500},\n",
    "                {\"product\": \"Washing Machine\", \"units_sold\": 124, \"price\": 900, \"revenue\": 111600},\n",
    "                {\"product\": \"Microwave\", \"units_sold\": 267, \"price\": 200, \"revenue\": 53400}\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Furniture\",\n",
    "            \"employees\": 28,\n",
    "            \"sales\": [\n",
    "                {\"product\": \"Desk\", \"units_sold\": 178, \"price\": 450, \"revenue\": 80100},\n",
    "                {\"product\": \"Chair\", \"units_sold\": 432, \"price\": 150, \"revenue\": 64800},\n",
    "                {\"product\": \"Bookshelf\", \"units_sold\": 95, \"price\": 300, \"revenue\": 28500}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save the data to a JSON file\n",
    "json_name = './complex_json.json'\n",
    "with open(json_name, 'w') as f:\n",
    "    json.dump(complex_json, f, indent=2)\n",
    "\n",
    "# Ask the LLM to analyze the attached JSON file\n",
    "analysis_query = \"\"\"\n",
    "Analyze the attached company data JSON file and tell me:\n",
    "1. Which department has the highest total revenue?\n",
    "2. What is the average revenue per employee across all departments?\n",
    "3. Which single product generated the most revenue?\n",
    "4. Calculate the total units sold across all departments.\n",
    "\n",
    "Please provide exact numbers and show your calculations.\n",
    "\"\"\"\n",
    "\n",
    "# Expected Answers:\n",
    "# 1. Electronics: 1,139,800\n",
    "# 2. 15,356.19 (1,615,300 total revenue / 105 total employees)\n",
    "# 3. Smartphone: 684,800\n",
    "# 4. 2,503 units total\n",
    "\n",
    "# NEW: Upload the JSON file instead of pasting data in prompt\n",
    "json_reply, _ = client.chat_new(\n",
    "    MODEL, \n",
    "    analysis_query,\n",
    "    files=[json_name]\n",
    ")\n",
    "\n",
    "print(\"\\n=== Complex JSON Analysis Response ===\")\n",
    "print(json_reply)\n",
    "\n",
    "# Cleanup\n",
    "Path(json_name).unlink()\n",
    "print(f\"\\n✓ Cleaned up {json_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13) Real Data Analysis - Warpage Statistics\n",
    "\n",
    "Analyze real manufacturing warpage measurement data from uploaded JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13) Real Data Analysis - Warpage Statistics from JSON\n",
    "# Load and analyze the actual warpage analysis report data\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "stats_path = Path(f\"data/uploads/{username}/20251013_stats.json\")\n",
    "with open(stats_path, 'r') as f:\n",
    "    warpage_stats = json.load(f)\n",
    "\n",
    "analysis_query = \"\"\"\n",
    "Based on this warpage analysis data with 50 measurement files, please analyze and tell me:\n",
    "\n",
    "1. Which file has the highest maximum warpage value and what is it?\n",
    "2. Which file has the lowest minimum warpage value and what is it?\n",
    "3. What is the average mean warpage across all 50 files?\n",
    "4. Calculate the overall standard deviation range (min std to max std) across all files\n",
    "5. Which file shows the most variability (highest range) and what is that range?\n",
    "6. What is the average kurtosis value across all measurements?\n",
    "7. Identify any files with extreme kurtosis (>47) which might indicate outliers\n",
    "\n",
    "Please provide specific file IDs and numeric values in your analysis.\n",
    "\"\"\"\n",
    "\n",
    "warpage_reply, _ = client.chat_new(MODEL, analysis_query, files=[stats_path])\n",
    "print(\"=== Warpage Statistics Analysis ===\")\n",
    "print(warpage_reply)\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14) Python Code Generation - Simple Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let the agent automatically generate and execute Python code\n",
    "calculation_query = \"\"\"\n",
    "Calculate the Fibonacci sequence up to 100.\n",
    "Use an efficient iterative approach and print the result as a JSON list.\n",
    "\n",
    "Along with the results, please provide the code.\n",
    "\"\"\"\n",
    "\n",
    "python_reply, _ = client.chat_new(MODEL, calculation_query)\n",
    "print(\"Python Code Generation Response:\")\n",
    "print(python_reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15) Python Code Generation - Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate code to analyze data with pandas\n",
    "data_analysis_query = \"\"\"\n",
    "Write Python code to:\n",
    "1. Create a pandas DataFrame with 100 rows of random sales data (date, product, quantity, price)\n",
    "2. Calculate total revenue per product\n",
    "3. Find the top 3 products by revenue\n",
    "4. Output results as JSON\n",
    "\n",
    "Use numpy for random data generation.\n",
    "\"\"\"\n",
    "\n",
    "data_reply, _ = client.chat_new(MODEL, data_analysis_query)\n",
    "print(\"Data Analysis Code Response:\")\n",
    "print(data_reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16) Python Code Generation - Mathematical Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate code for complex mathematical calculations\n",
    "math_query = \"\"\"\n",
    "Write Python code to:\n",
    "1. Calculate the first 20 prime numbers\n",
    "2. Compute their sum and average\n",
    "3. Find the largest prime in the list\n",
    "4. Output results as JSON with keys: primes, sum, average, largest\n",
    "\n",
    "Show the code.\n",
    "\"\"\"\n",
    "\n",
    "math_reply, _ = client.chat_new(MODEL, math_query, agent_type=\"auto\")\n",
    "print(\"Mathematical Computation Response:\")\n",
    "print(math_reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 17) Python Code Generation - String Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate code for text analysis\n",
    "text_query = \"\"\"\n",
    "Write Python code to analyze the following text:\n",
    "\"The quick brown fox jumps over the lazy dog. The dog was not amused.\"\n",
    "\n",
    "Calculate:\n",
    "1. Total word count\n",
    "2. Unique word count\n",
    "3. Most frequent word\n",
    "4. Average word length\n",
    "5. Output as JSON\n",
    "\"\"\"\n",
    "\n",
    "text_reply, _ = client.chat_new(MODEL, text_query, agent_type=\"auto\")\n",
    "print(\"Text Processing Response:\")\n",
    "print(text_reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18) Python Code Generation - Excel File Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19) File Upload with Chat - CSV Analysis\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=== Testing File Upload with Chat ===\\n\")\n",
    "\n",
    "# Create test CSV file\n",
    "test_data = pd.DataFrame({\n",
    "    \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\"],\n",
    "    \"age\": [30, 25, 35, 28, 32],\n",
    "    \"city\": [\"Seoul\", \"Busan\", \"Incheon\", \"Daegu\", \"Seoul\"],\n",
    "    \"salary\": [50000, 45000, 55000, 48000, 52000]\n",
    "})\n",
    "test_csv_path = \"test_employee_data.csv\"\n",
    "test_data.to_csv(test_csv_path, index=False)\n",
    "print(f\"✓ Created test file: {test_csv_path}\")\n",
    "print(f\"  Data shape: {test_data.shape}\")\n",
    "\n",
    "# Send chat with file attachment\n",
    "query = \"\"\"\n",
    "Analyze the attached employee CSV file and tell me:\n",
    "1. Average age and salary\n",
    "2. City with most employees\n",
    "3. Highest and lowest salary\n",
    "4. Any interesting patterns\n",
    "\"\"\"\n",
    "\n",
    "reply, session_id = client.chat_new(\n",
    "    model=MODEL,\n",
    "    user_message=query,\n",
    "    agent_type=\"auto\",\n",
    "    files=[test_csv_path]\n",
    ")\n",
    "\n",
    "print(\"\\n=== AI Response ===\")\n",
    "print(reply)\n",
    "print(f\"\\nSession ID: {session_id}\")\n",
    "\n",
    "# Cleanup\n",
    "Path(test_csv_path).unlink()\n",
    "print(f\"\\n✓ Cleaned up test file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 19) File Upload with Chat - CSV Data Analysis\n",
    "\n",
    "Test the new file upload feature by creating and analyzing a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18) Python Code Generation - Excel File Analysis (with File Upload)\n",
    "# Now using the new file attachment feature!\n",
    "\n",
    "excel_path = f\"data/uploads/{username}/폴드긍정.xlsx\"\n",
    "\n",
    "excel_analysis_query = \"\"\"\n",
    "Analyze the attached Excel file and:\n",
    "1. Display the column names\n",
    "2. Show the first 5 rows\n",
    "3. Calculate basic statistics (count, mean, std) for numeric columns\n",
    "4. Output all results as a well-formatted JSON\n",
    "\n",
    "Make sure to handle Korean text encoding properly.\n",
    "\"\"\"\n",
    "\n",
    "# NEW: Upload the file with the chat request\n",
    "excel_reply, _ = client.chat_new(\n",
    "    MODEL, \n",
    "    excel_analysis_query, \n",
    "    agent_type=\"auto\",\n",
    "    files=[excel_path]  # Attach the file!\n",
    ")\n",
    "\n",
    "print(\"Excel File Analysis Response:\")\n",
    "print(excel_reply)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
