{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM API – End-to-End Examples (Single Notebook)\n",
    "\n",
    "This notebook shows a minimal client and step-by-step examples for:\n",
    "\n",
    "1. Create a new account\n",
    "2. Login\n",
    "3. Change models (admin privilege example)\n",
    "4. Start a new chat and get a response\n",
    "5. Continue a chat\n",
    "6. See chat history\n",
    "7. Websearch with agentic tool selection\n",
    "8. Agentic math calculation (LLM decides to use math tool)\n",
    "9. Sequential reasoning with ReAct agent (step-by-step thinking)\n",
    "10. Plan-and-Execute agent (parallel tool usage)\n",
    "11. Auto agent selection (smart router picks best agent)\n",
    "12. Complex JSON data analysis\n",
    "13. Real Data Analysis - Warpage Statistics (using 20251013_stats.json)\n",
    "14. Python Code Generation - Simple Calculation\n",
    "15. Python Code Generation - Data Analysis\n",
    "16. Python Code Generation - Mathematical Computation\n",
    "17. Python Code Generation - String Processing\n",
    "18. Python Code Generation - Excel File Analysis (Real File)\n",
    "\n",
    "Set your API base URL below if different from the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: httpx in c:\\users\\lee\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.28.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\lee\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\lee\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\lee\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\lee\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\lee\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\lee\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from anyio->httpx) (1.3.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5 in c:\\users\\lee\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from anyio->httpx) (4.15.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip-system-certs in c:\\users\\lee\\appdata\\roaming\\python\\python313\\site-packages (5.3)\n",
      "Requirement already satisfied: pip>=24.2 in c:\\python313\\lib\\site-packages (from pip-system-certs) (25.1.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~uitka (C:\\Users\\Lee\\AppData\\Roaming\\Python\\Python313\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~uitka (C:\\Users\\Lee\\AppData\\Roaming\\Python\\Python313\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~uitka (C:\\Users\\Lee\\AppData\\Roaming\\Python\\Python313\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~uitka (C:\\Users\\Lee\\AppData\\Roaming\\Python\\Python313\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~uitka (C:\\Users\\Lee\\AppData\\Roaming\\Python\\Python313\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: C:\\Python313\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install httpx\n",
    "! pip install pip-system-certs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: http://localhost:1007\n"
     ]
    }
   ],
   "source": [
    "API_BASE_URL = \"http://10.252.38.241:1007\"\n",
    "API_BASE_URL = 'http://localhost:1007'\n",
    "print(\"Using:\", API_BASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client ready with 1200s (20 min) timeout for chat requests\n",
      "✓ Now supports multipart/form-data with optional file attachments\n"
     ]
    }
   ],
   "source": [
    "import httpx\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "class LLMApiClient:\n",
    "    def __init__(self, base_url: str, timeout: float = 1200.0):\n",
    "        \"\"\"\n",
    "        Initialize the LLM API client.\n",
    "\n",
    "        Args:\n",
    "            base_url: API base URL\n",
    "            timeout: Request timeout in seconds (default: 1200s/20min for LLM requests)\n",
    "        \"\"\"\n",
    "        self.base_url = base_url.rstrip(\"/\")\n",
    "        self.token = None\n",
    "        # Create timeout config: 10s for connect, custom timeout for read/write/pool\n",
    "        self.timeout = httpx.Timeout(50.0, read=timeout, write=timeout, pool=timeout)\n",
    "\n",
    "    def _headers(self):\n",
    "        # Don't set Content-Type - httpx auto-sets for multipart\n",
    "        h = {}\n",
    "        if self.token:\n",
    "            h[\"Authorization\"] = f\"Bearer {self.token}\"\n",
    "        return h\n",
    "\n",
    "    def signup(self, username: str, password: str, role: str = \"guest\"):\n",
    "        r = httpx.post(f\"{self.base_url}/api/auth/signup\", json={\n",
    "            \"username\": username, \"password\": password, \"role\": role\n",
    "        }, timeout=10.0)\n",
    "        r.raise_for_status()\n",
    "        return r.json()\n",
    "\n",
    "    def login(self, username: str, password: str):\n",
    "        r = httpx.post(f\"{self.base_url}/api/auth/login\", json={\n",
    "            \"username\": username, \"password\": password\n",
    "        }, timeout=10.0)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        self.token = data[\"access_token\"]\n",
    "        return data\n",
    "\n",
    "    def list_models(self):\n",
    "        # JSON endpoints still use Content-Type header\n",
    "        headers = {\"Authorization\": f\"Bearer {self.token}\"} if self.token else {}\n",
    "        r = httpx.get(f\"{self.base_url}/v1/models\", headers=headers, timeout=10.0)\n",
    "        r.raise_for_status()\n",
    "        return r.json()\n",
    "\n",
    "    def change_model(self, model: str):\n",
    "        headers = {\"Authorization\": f\"Bearer {self.token}\", \"Content-Type\": \"application/json\"} if self.token else {\"Content-Type\": \"application/json\"}\n",
    "        r = httpx.post(f\"{self.base_url}/api/admin/model\", json={\"model\": model}, headers=headers, timeout=10.0)\n",
    "        r.raise_for_status()\n",
    "        return r.json()\n",
    "\n",
    "    def chat_new(self, model: str, user_message: str, agent_type: str = \"auto\", files: list = None):\n",
    "        \"\"\"\n",
    "        Start new chat with optional file attachments (multipart/form-data)\n",
    "        \n",
    "        Args:\n",
    "            model: Model name\n",
    "            user_message: User message\n",
    "            agent_type: Agent type (auto, react, plan_execute)\n",
    "            files: Optional list of file paths to attach\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (response_text, session_id)\n",
    "        \"\"\"\n",
    "        messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "        \n",
    "        # Prepare form data\n",
    "        data = {\n",
    "            \"model\": model,\n",
    "            \"messages\": json.dumps(messages),\n",
    "            \"agent_type\": agent_type\n",
    "        }\n",
    "        \n",
    "        # Prepare files for upload\n",
    "        files_to_upload = []\n",
    "        if files:\n",
    "            for file_path in files:\n",
    "                f = open(file_path, \"rb\")\n",
    "                files_to_upload.append((\"files\", (Path(file_path).name, f)))\n",
    "        \n",
    "        try:\n",
    "            r = httpx.post(\n",
    "                f\"{self.base_url}/v1/chat/completions\",\n",
    "                data=data,\n",
    "                files=files_to_upload if files_to_upload else None,\n",
    "                headers=self._headers(),\n",
    "                timeout=self.timeout\n",
    "            )\n",
    "            r.raise_for_status()\n",
    "            result = r.json()\n",
    "            return result[\"choices\"][0][\"message\"][\"content\"], result[\"x_session_id\"]\n",
    "        \n",
    "        finally:\n",
    "            # Close file handles\n",
    "            for _, (_, f) in files_to_upload:\n",
    "                f.close()\n",
    "\n",
    "    def chat_continue(self, model: str, session_id: str, user_message: str, agent_type: str = \"auto\", files: list = None):\n",
    "        \"\"\"\n",
    "        Continue existing chat with optional file attachments (multipart/form-data)\n",
    "        \n",
    "        Args:\n",
    "            model: Model name\n",
    "            session_id: Session ID to continue\n",
    "            user_message: User message\n",
    "            agent_type: Agent type (auto, react, plan_execute)\n",
    "            files: Optional list of file paths to attach\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (response_text, session_id)\n",
    "        \"\"\"\n",
    "        messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "        \n",
    "        data = {\n",
    "            \"model\": model,\n",
    "            \"messages\": json.dumps(messages),\n",
    "            \"session_id\": session_id,\n",
    "            \"agent_type\": agent_type\n",
    "        }\n",
    "        \n",
    "        files_to_upload = []\n",
    "        if files:\n",
    "            for file_path in files:\n",
    "                f = open(file_path, \"rb\")\n",
    "                files_to_upload.append((\"files\", (Path(file_path).name, f)))\n",
    "        \n",
    "        try:\n",
    "            r = httpx.post(\n",
    "                f\"{self.base_url}/v1/chat/completions\",\n",
    "                data=data,\n",
    "                files=files_to_upload if files_to_upload else None,\n",
    "                headers=self._headers(),\n",
    "                timeout=self.timeout\n",
    "            )\n",
    "            r.raise_for_status()\n",
    "            result = r.json()\n",
    "            return result[\"choices\"][0][\"message\"][\"content\"], result[\"x_session_id\"]\n",
    "        \n",
    "        finally:\n",
    "            for _, (_, f) in files_to_upload:\n",
    "                f.close()\n",
    "\n",
    "    def chat_sessions(self):\n",
    "        r = httpx.get(f\"{self.base_url}/api/chat/sessions\", headers=self._headers(), timeout=10.0)\n",
    "        r.raise_for_status()\n",
    "        return r.json()[\"sessions\"]\n",
    "\n",
    "    def chat_history(self, session_id: str):\n",
    "        r = httpx.get(f\"{self.base_url}/api/chat/history/{session_id}\", headers=self._headers(), timeout=10.0)\n",
    "        r.raise_for_status()\n",
    "        return r.json()[\"messages\"]\n",
    "\n",
    "    def tools(self):\n",
    "        r = httpx.get(f\"{self.base_url}/api/tools/list\", headers=self._headers(), timeout=10.0)\n",
    "        r.raise_for_status()\n",
    "        return r.json()[\"tools\"]\n",
    "\n",
    "    def websearch(self, query: str, max_results: int = 5):\n",
    "        \"\"\"\n",
    "        Perform web search and get LLM-generated answer\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with:\n",
    "            - answer: LLM-generated answer from search results\n",
    "            - results: Raw search results (list of dicts with title, url, content, score)\n",
    "            - sources_used: List of URLs used as sources\n",
    "        \"\"\"\n",
    "        headers = {\"Authorization\": f\"Bearer {self.token}\", \"Content-Type\": \"application/json\"} if self.token else {\"Content-Type\": \"application/json\"}\n",
    "        r = httpx.post(f\"{self.base_url}/api/tools/websearch\", json={\"query\": query, \"max_results\": max_results}, headers=headers, timeout=60.0)\n",
    "        r.raise_for_status()\n",
    "        return r.json()  # Returns full response with answer, results, and sources_used\n",
    "\n",
    "    def answer_from_json(self, model: str, json_blob: dict, question: str):\n",
    "        prompt = f\"Given this JSON: {json_blob}\\nAnswer: {question}\"\n",
    "        return self.chat_new(model, prompt)[0]\n",
    "\n",
    "client = LLMApiClient(API_BASE_URL, timeout=12000.0)  # 20 minute timeout\n",
    "print(\"Client ready with 1200s (20 min) timeout for chat requests\")\n",
    "print(\"✓ Now supports multipart/form-data with optional file attachments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Create a new account (skip if user already exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signup skipped (user may already exist): Client error '400 Bad Request' for url 'http://localhost:1007/api/auth/signup'\n",
      "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/400\n",
      "Continuing with existing account...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "username = \"leesihun\"\n",
    "password = \"s.hun.lee\"\n",
    "try:\n",
    "    result = client.signup(username, password)\n",
    "    print(f\"Account created: {result}\")\n",
    "except Exception as e:\n",
    "    print(f\"Signup skipped (user may already exist): {e}\")\n",
    "    print(\"Continuing with existing account...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'access_token': 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJsZWVzaWh1biIsImV4cCI6MTc2MjQwMTUwMH0.Xbftfo6h_3w9cPwnb-vU2V0cXbIkaTqWLYOGBRShQ-0',\n",
       " 'token_type': 'bearer',\n",
       " 'user': {'username': 'leesihun', 'role': 'guest'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "login = client.login(username, password)\n",
    "login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Change models (admin only) – optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'success': True, 'model': 'deepseek-r1:1.5b'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using deepseek-r1:1.5b for all examples\n",
    "client.login(\"admin\", \"administrator\")\n",
    "client.change_model(\"gpt-oss:20b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'object': 'list',\n",
       " 'data': [{'id': 'deepseek-r1:1.5b',\n",
       "   'object': 'model',\n",
       "   'created': 1762315117,\n",
       "   'owned_by': 'ollama'}]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List models (OpenAI-compatible)\n",
    "models = client.list_models()\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Start a new chat and get a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"Red leaves gather in my hand,  \\nA dance of旋云 around them fall.  \\nSomeone stands where dust sways,  \\nIn a gentle hand,  \\nThe ground feels warm,  \\nAs if nature tells me its song's time's done.\",\n",
       " 'be5d35f8-3d25-4e9c-a560-4cd55383d3d6')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "MODEL = models[\"data\"][0][\"id\"]\n",
    "reply, session_id = client.chat_new(MODEL, \"Hello! Give me a short haiku about autumn.\")\n",
    "reply, session_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Continue an existing chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The ice begins,  \\n melted firelight falls,  \\nLushes freeze,  \\nwind whispers past.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "reply2, _ = client.chat_continue(MODEL, session_id, \"Now do one about winter.\")\n",
    "reply2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) See chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['be5d35f8-3d25-4e9c-a560-4cd55383d3d6'],\n",
       " [{'role': 'user',\n",
       "   'content': 'Hello! Give me a short haiku about autumn.',\n",
       "   'timestamp': '2025-11-05T12:58:57.635028+09:00',\n",
       "   'metadata': {'file_paths': None, 'task_type': 'chat'}},\n",
       "  {'role': 'assistant',\n",
       "   'content': \"Red leaves gather in my hand,  \\nA dance of旋云 around them fall.  \\nSomeone stands where dust sways,  \\nIn a gentle hand,  \\nThe ground feels warm,  \\nAs if nature tells me its song's time's done.\",\n",
       "   'timestamp': '2025-11-05T12:58:57.645044+09:00',\n",
       "   'metadata': {'task_type': 'chat'}},\n",
       "  {'role': 'user',\n",
       "   'content': 'Now do one about winter.',\n",
       "   'timestamp': '2025-11-05T13:00:27.623224+09:00',\n",
       "   'metadata': {'file_paths': None, 'task_type': 'chat'}},\n",
       "  {'role': 'assistant',\n",
       "   'content': 'The ice begins,  \\n melted firelight falls,  \\nLushes freeze,  \\nwind whispers past.',\n",
       "   'timestamp': '2025-11-05T13:00:27.634411+09:00',\n",
       "   'metadata': {'task_type': 'chat'}}])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "client.chat_sessions(), client.chat_history(session_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7) Websearch with LLM-generated answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LLM-Generated Answer ===\n",
      "According to Source 2, Sihun Lee is a student at Washington University in St. Louis. However, the specific details about her work or achievements are not available from other sources such as her LinkedIn profile or professional activities.\n",
      "\n",
      "**Citations:**\n",
      "Source 1 and 3 provide background on Sihun Lee, including information about her educational institutions and a brief description of her life. Source 2 offers more detailed information about her academic journey and university, while Sources 4 provides additional context on her name and birth details, though they do not focus on her professional work or achievements.\n",
      "\n",
      "=== Sources Used ===\n",
      "1. https://www.linkedin.com/in/sihunlee0604\n",
      "2. https://www.linkedin.com/in/sihun-lee-97a32518a\n",
      "3. https://en.namu.wiki/w/%EC%9D%B4%EC%8B%9C%ED%9B%88\n",
      "4. https://asianwiki.com/Lee_Si-Hun_(1984)\n",
      "5. https://www.researchgate.net/profile/Sihun-Lee\n",
      "\n",
      "=== Raw Search Results (for reference) ===\n",
      "[{'content': 'Sihun Lee\\n'\n",
      "             'J.P. Morgan\\n'\n",
      "             'New York, New York, United States\\n'\n",
      "             '500 connections, 1900 followers\\n'\n",
      "             '\\n'\n",
      "             '\\n'\n",
      "             'About\\n'\n",
      "             'N/A\\n'\n",
      "             '\\n'\n",
      "             '\\n'\n",
      "             'Experience\\n'\n",
      "             'J.P. Morgan\\n'\n",
      "             '[J.P. Morgan](N/A)  \\n'\n",
      "             'N/A - Present\\n'\n",
      "             'N/A\\n'\n",
      "             '\\n'\n",
      "             '\\n'\n",
      "             '\\n'\n",
      "             'Education\\n'\n",
      "             'N/A',\n",
      "  'score': 0.9851,\n",
      "  'title': 'Sihun Lee - J.P. Morgan - LinkedIn',\n",
      "  'url': 'https://www.linkedin.com/in/sihunlee0604'},\n",
      " {'content': 'Sihun Lee\\n'\n",
      "             'Student at Washington University in St. Louis\\n'\n",
      "             'St Louis, Missouri, United States\\n'\n",
      "             '242 connections, 242 followers\\n'\n",
      "             '\\n'\n",
      "             '\\n'\n",
      "             'About\\n'\n",
      "             'N/A\\n'\n",
      "             '\\n'\n",
      "             '\\n'\n",
      "             'Experience\\n'\n",
      "             'Research Assistant\\n'\n",
      "             '[Washington University in St. '\n",
      "             'Louis](https://www.linkedin.com/school/washington-university-in-st-louis/)  \\n'\n",
      "             'Jan 2025 - Present\\n'\n",
      "             'St Louis, Missouri, United States\\n'\n",
      "             'Personality Measurement and Development Lab\\n'\n",
      "             '\\n'\n",
      "             '\\n'\n",
      "             'Education\\n'\n",
      "             'Washington University in St. Louis\\n'\n",
      "             'Psychological & Brain Sciences\\n'\n",
      "             '2020 - 2026\\n'\n",
      "             'None',\n",
      "  'score': 0.98454,\n",
      "  'title': 'Sihun Lee - Student at Washington University in St. Louis | '\n",
      "           'LinkedIn',\n",
      "  'url': 'https://www.linkedin.com/in/sihun-lee-97a32518a'},\n",
      " {'content': 'Sihun Lee · Born in 1984 · Debut in 2016 · korean actor · '\n",
      "             'Children of Korean voice actors · Rael BNC artist. More. '\n",
      "             'File:Rael B & C logo 02.Png.',\n",
      "  'score': 0.98013,\n",
      "  'title': 'Sihun Lee - NamuWiki',\n",
      "  'url': 'https://en.namu.wiki/w/%EC%9D%B4%EC%8B%9C%ED%9B%88'},\n",
      " {'content': 'Profile · Name: Lee Si-Hun · Hangul: 이시훈 · Born: August 25, 1984 '\n",
      "             '· Birthplace: South Korea · Height: 177 cm · Blood Type: · '\n",
      "             'Family: Lim Yun-Bi',\n",
      "  'score': 0.97218,\n",
      "  'title': 'Lee Si-Hun (1984) - AsianWiki',\n",
      "  'url': 'https://asianwiki.com/Lee_Si-Hun_(1984)'}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# The API now generates an answer from search results using LLM\n",
    "client.login(\"leesihun\", \"s.hun.lee\")\n",
    "search_query = \"Tell me who is SiHun Lee\"\n",
    "search_response = client.websearch(search_query)\n",
    "\n",
    "print(\"=== LLM-Generated Answer ===\")\n",
    "print(search_response[\"answer\"])\n",
    "print(\"\\n=== Sources Used ===\")\n",
    "for i, url in enumerate(search_response[\"sources_used\"], 1):\n",
    "    print(f\"{i}. {url}\")\n",
    "print(\"\\n=== Raw Search Results (for reference) ===\")\n",
    "import pprint\n",
    "pprint.pprint(search_response[\"results\"][:4])  # Show first 2 raw results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7b) Websearch example - Sports news\n",
    "# Another example showing the LLM answer generation\n",
    "client.login(\"leesihun\", \"s.hun.lee\")\n",
    "search_query = \"What was the latest game of Liverpool FC and who won?\"\n",
    "search_response = client.websearch(search_query)\n",
    "\n",
    "print(\"=== LLM-Generated Answer ===\")\n",
    "print(search_response[\"answer\"])\n",
    "print(\"\\n=== Sources ===\")\n",
    "for i, url in enumerate(search_response[\"sources_used\"], 1):\n",
    "    print(f\"{i}. {url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8) Agentic tool usage - Let the LLM decide which tool to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Math Question Response:\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "To find \\( \\frac{11.951}{3.751} \\), follow these steps:\n",
       "\n",
       "### Step 1: Eliminate Decimals\n",
       "To simplify the division, eliminate the decimals by multiplying both the numerator and denominator by 10,000:\n",
       "\n",
       "\\[\n",
       "3.751 \\times 10,000 = 37,510 \\\\\n",
       "11.951 \\times 10,000 = 119,510\n",
       "\\]\n",
       "\n",
       "So, the problem becomes:\n",
       "\n",
       "\\[\n",
       "\\frac{119510}{3751}\n",
       "\\]\n",
       "\n",
       "### Step 2: Perform Long Division\n",
       "Divide 119,510 by 3,751 using long division:\n",
       "\n",
       "```\n",
       "    ___________\n",
       "3751 | 119510\n",
       "      - 3751 * 3 = 11,253\n",
       "        ----------------\n",
       "            6980\n",
       "           - 3751 * 1 = 3,751\n",
       "             -------\n",
       "               3229\n",
       "              - 3751 * 0 = 0\n",
       "                ------\n",
       "                  3229\n",
       "```\n",
       "\n",
       "After performing the division, you'll find that:\n",
       "\n",
       "\\[\n",
       "\\frac{119510}{3751} \\approx 31.64\n",
       "\\]\n",
       "\n",
       "### Final Answer\n",
       "\n",
       "\\[\n",
       "\\boxed{31.64}\n",
       "\\]"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To find \\( \\frac{11.951}{3.751} \\), follow these steps:\n",
      "\n",
      "### Step 1: Eliminate Decimals\n",
      "To simplify the division, eliminate the decimals by multiplying both the numerator and denominator by 10,000:\n",
      "\n",
      "\\[\n",
      "3.751 \\times 10,000 = 37,510 \\\\\n",
      "11.951 \\times 10,000 = 119,510\n",
      "\\]\n",
      "\n",
      "So, the problem becomes:\n",
      "\n",
      "\\[\n",
      "\\frac{119510}{3751}\n",
      "\\]\n",
      "\n",
      "### Step 2: Perform Long Division\n",
      "Divide 119,510 by 3,751 using long division:\n",
      "\n",
      "```\n",
      "    ___________\n",
      "3751 | 119510\n",
      "      - 3751 * 3 = 11,253\n",
      "        ----------------\n",
      "            6980\n",
      "           - 3751 * 1 = 3,751\n",
      "             -------\n",
      "               3229\n",
      "              - 3751 * 0 = 0\n",
      "                ------\n",
      "                  3229\n",
      "```\n",
      "\n",
      "After performing the division, you'll find that:\n",
      "\n",
      "\\[\n",
      "\\frac{119510}{3751} \\approx 31.64\n",
      "\\]\n",
      "\n",
      "### Final Answer\n",
      "\n",
      "\\[\n",
      "\\boxed{31.64}\n",
      "\\]\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# Simple math question (agent will automatically use math_calculator tool)\n",
    "math_reply, _ = client.chat_new(MODEL, \"What is 11.951/3.751?\", agent_type='react')\n",
    "print(\"Math Question Response:\")\n",
    "from IPython.display import display, Math, Latex\n",
    "display(Latex(math_reply))\n",
    "print(math_reply)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9) Sequential reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This triggers the ReAct agent because it requires step-by-step thinking\n",
    "sequential_query = \"\"\"\n",
    "First, search the web to find the latest population of Tokyo.\n",
    "Then, calculate what 15% of that population would be.\n",
    "Finally, tell me the result.\n",
    "Think hard, try to answer to best of your knowledge\n",
    "\"\"\"\n",
    "react_reply, _ = client.chat_new(MODEL, sequential_query)\n",
    "print(\"Sequential Reasoning (ReAct) Response:\")\n",
    "\n",
    "display(Latex(react_reply))\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10) Plan-and-Execute agent with multiple tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This triggers Plan-and-Execute agent because it uses \"and\" for parallel tasks\n",
    "parallel_query = \"\"\"\n",
    "Search for the latest news about artificial intelligence and\n",
    "calculate the result of (100 * 0.15 + 25) / 2 and\n",
    "Think about what god is and\n",
    "What the best smart phone is and\n",
    "what is 1007*1007/4524753.\n",
    "\"\"\"\n",
    "plan_reply, _ = client.chat_new(MODEL, parallel_query, agent_type=\"plan_execute\")\n",
    "print(\"Plan-and-Execute Response:\")\n",
    "\n",
    "display(Latex(plan_reply))\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11) Auto agent selection - Let the router decide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The smart router will analyze the query and pick the best agent\n",
    "auto_query = \"If the capital of France has a population of 2.1 million, and we need to allocate 500 euros per person for a project, what's the total budget needed? First search for the actual population, then calculate.\"\n",
    "auto_reply, _ = client.chat_new(MODEL, auto_query, agent_type=\"auto\")\n",
    "print(\"Auto Agent Selection Response:\")\n",
    "print(auto_reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12) Complex JSON data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12) Complex JSON data analysis (with File Upload)\n",
    "import json\n",
    "\n",
    "# Create a realistic e-commerce dataset\n",
    "complex_json = {\n",
    "    \"company\": \"TechMart Inc\",\n",
    "    \"quarter\": \"Q3 2025\",\n",
    "    \"departments\": [\n",
    "        {\n",
    "            \"name\": \"Electronics\",\n",
    "            \"employees\": 45,\n",
    "            \"sales\": [\n",
    "                {\"product\": \"Laptop\", \"units_sold\": 320, \"price\": 1200, \"revenue\": 384000},\n",
    "                {\"product\": \"Smartphone\", \"units_sold\": 856, \"price\": 800, \"revenue\": 684800},\n",
    "                {\"product\": \"Tablet\", \"units_sold\": 142, \"price\": 500, \"revenue\": 71000}\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Home Appliances\",\n",
    "            \"employees\": 32,\n",
    "            \"sales\": [\n",
    "                {\"product\": \"Refrigerator\", \"units_sold\": 89, \"price\": 1500, \"revenue\": 133500},\n",
    "                {\"product\": \"Washing Machine\", \"units_sold\": 124, \"price\": 900, \"revenue\": 111600},\n",
    "                {\"product\": \"Microwave\", \"units_sold\": 267, \"price\": 200, \"revenue\": 53400}\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Furniture\",\n",
    "            \"employees\": 28,\n",
    "            \"sales\": [\n",
    "                {\"product\": \"Desk\", \"units_sold\": 178, \"price\": 450, \"revenue\": 80100},\n",
    "                {\"product\": \"Chair\", \"units_sold\": 432, \"price\": 150, \"revenue\": 64800},\n",
    "                {\"product\": \"Bookshelf\", \"units_sold\": 95, \"price\": 300, \"revenue\": 28500}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save the data to a JSON file\n",
    "json_name = './complex_json.json'\n",
    "with open(json_name, 'w') as f:\n",
    "    json.dump(complex_json, f, indent=2)\n",
    "\n",
    "# Ask the LLM to analyze the attached JSON file\n",
    "analysis_query = \"\"\"\n",
    "Analyze the attached company data JSON file and tell me:\n",
    "1. Which department has the highest total revenue?\n",
    "2. What is the average revenue per employee across all departments?\n",
    "3. Which single product generated the most revenue?\n",
    "4. Calculate the total units sold across all departments.\n",
    "\n",
    "Please provide exact numbers and show your calculations.\n",
    "\"\"\"\n",
    "\n",
    "# Expected Answers:\n",
    "# 1. Electronics: 1,139,800\n",
    "# 2. 15,356.19 (1,615,300 total revenue / 105 total employees)\n",
    "# 3. Smartphone: 684,800\n",
    "# 4. 2,503 units total\n",
    "\n",
    "# NEW: Upload the JSON file instead of pasting data in prompt\n",
    "json_reply, _ = client.chat_new(\n",
    "    MODEL, \n",
    "    analysis_query,\n",
    "    files=[json_name]\n",
    ")\n",
    "\n",
    "print(\"\\n=== Complex JSON Analysis Response ===\")\n",
    "print(json_reply)\n",
    "\n",
    "# Cleanup\n",
    "Path(json_name).unlink()\n",
    "print(f\"\\n✓ Cleaned up {json_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13) Real Data Analysis - Warpage Statistics\n",
    "\n",
    "Analyze real manufacturing warpage measurement data from uploaded JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13) Real Data Analysis - Warpage Statistics from JSON\n",
    "# Load and analyze the actual warpage analysis report data\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "stats_path = Path(f\"data/uploads/{username}/20251013_stats.json\")\n",
    "with open(stats_path, 'r') as f:\n",
    "    warpage_stats = json.load(f)\n",
    "\n",
    "analysis_query = \"\"\"\n",
    "The given file contains 50 measurements of warpage.\n",
    "Based on this warpage analysis data, please analyze and tell me:\n",
    "\n",
    "1. Which has the highest maximum warpage value and what is it?\n",
    "2. Which has the lowest minimum warpage value and what is it?\n",
    "3. What is the average mean warpage across all 50 measurements?\n",
    "4. Calculate the overall standard deviation range (min std to max std) across all measurements\n",
    "5. Which measurement shows the most variability (highest range) and what is that range?\n",
    "6. What is the average kurtosis value across all measurements?\n",
    "7. Identify any files with extreme kurtosis (>47) which might indicate outliers\n",
    "\n",
    "Please provide specific file IDs and numeric values in your analysis.\n",
    "\"\"\"\n",
    "\n",
    "warpage_reply, _ = client.chat_new(MODEL, analysis_query, files=[stats_path])\n",
    "print(\"=== Warpage Statistics Analysis ===\")\n",
    "print(warpage_reply)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "\"\"\"\n",
    "Answer: \n",
    "\n",
    "\n",
    "1. Highest Maximum Warpage Value:\n",
    "File ID: File_48\n",
    "Filename: 20251013146640@B5913326505D7_ORI.txt\n",
    "Maximum Value: 534.0\n",
    "2. Lowest Minimum Warpage Value:\n",
    "File ID: File_04\n",
    "Filename: 20251013142156@B5913326505D7_ORI.txt\n",
    "Minimum Value: -4200.0\n",
    "3. Average Mean Warpage:\n",
    "-297.84 (across all 50 measurements)\n",
    "4. Overall Standard Deviation Range:\n",
    "Minimum Std: 71.23\n",
    "Maximum Std: 87.89\n",
    "Range: 16.66\n",
    "5. Most Variability (Highest Range):\n",
    "File ID: File_04\n",
    "Filename: 20251013142156@B5913326505D7_ORI.txt\n",
    "Range: 4723.0\n",
    "6. Average Kurtosis Value:\n",
    "44.43 (across all measurements)\n",
    "7. Files with Extreme Kurtosis (>47) - Potential Outliers: Found 7 files with extreme kurtosis:\n",
    "File ID\tFilename\tKurtosis\n",
    "File_48\t20251013146640@...\t48.35\n",
    "File_36\t20251013145428@...\t48.23\n",
    "File_24\t20251013144216@...\t48.12\n",
    "File_04\t20251013142156@...\t47.89\n",
    "File_40\t20251013145832@...\t47.57\n",
    "File_28\t20251013144620@...\t47.46\n",
    "File_12\t20251013143004@...\t47.12\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14) Python Code Generation - Simple Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let the agent automatically generate and execute Python code\n",
    "calculation_query = \"\"\"\n",
    "Calculate the Fibonacci sequence up to 100.\n",
    "Use an efficient iterative approach and print the result as a JSON list.\n",
    "\n",
    "Along with the results, please provide the code.\n",
    "\"\"\"\n",
    "\n",
    "python_reply, _ = client.chat_new(MODEL, calculation_query)\n",
    "print(\"Python Code Generation Response:\")\n",
    "print(python_reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15) Python Code Generation - Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate code to analyze data with pandas\n",
    "data_analysis_query = \"\"\"\n",
    "Write Python code to:\n",
    "1. Create a pandas DataFrame with 100 rows of random sales data (date, product, quantity, price)\n",
    "2. Calculate total revenue per product\n",
    "3. Find the top 3 products by revenue\n",
    "4. Output results as JSON\n",
    "\n",
    "Use numpy for random data generation.\n",
    "\"\"\"\n",
    "\n",
    "data_reply, _ = client.chat_new(MODEL, data_analysis_query)\n",
    "print(\"Data Analysis Code Response:\")\n",
    "print(data_reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16) Python Code Generation - Mathematical Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate code for complex mathematical calculations\n",
    "math_query = \"\"\"\n",
    "Write Python code to:\n",
    "1. Calculate the first 20 prime numbers\n",
    "2. Compute their sum and average\n",
    "3. Find the largest prime in the list\n",
    "4. Output results as JSON with keys: primes, sum, average, largest\n",
    "\n",
    "Show the code.\n",
    "\"\"\"\n",
    "\n",
    "math_reply, _ = client.chat_new(MODEL, math_query, agent_type=\"auto\")\n",
    "print(\"Mathematical Computation Response:\")\n",
    "print(math_reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 17) Python Code Generation - String Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate code for text analysis\n",
    "text_query = \"\"\"\n",
    "Write Python code to analyze the following text:\n",
    "\"The quick brown fox jumps over the lazy dog. The dog was not amused.\"\n",
    "\n",
    "Calculate:\n",
    "1. Total word count\n",
    "2. Unique word count\n",
    "3. Most frequent word\n",
    "4. Average word length\n",
    "5. Output as JSON\n",
    "\"\"\"\n",
    "\n",
    "text_reply, _ = client.chat_new(MODEL, text_query, agent_type=\"auto\")\n",
    "print(\"Text Processing Response:\")\n",
    "print(text_reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18) Python Code Generation - Excel File Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19) File Upload with Chat - CSV Analysis\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=== Testing File Upload with Chat ===\\n\")\n",
    "\n",
    "# Create test CSV file\n",
    "test_data = pd.DataFrame({\n",
    "    \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\"],\n",
    "    \"age\": [30, 25, 35, 28, 32],\n",
    "    \"city\": [\"Seoul\", \"Busan\", \"Incheon\", \"Daegu\", \"Seoul\"],\n",
    "    \"salary\": [50000, 45000, 55000, 48000, 52000]\n",
    "})\n",
    "test_csv_path = \"test_employee_data.csv\"\n",
    "test_data.to_csv(test_csv_path, index=False)\n",
    "print(f\"✓ Created test file: {test_csv_path}\")\n",
    "print(f\"  Data shape: {test_data.shape}\")\n",
    "\n",
    "# Send chat with file attachment\n",
    "query = \"\"\"\n",
    "Analyze the attached employee CSV file and tell me:\n",
    "1. Average age and salary\n",
    "2. City with most employees\n",
    "3. Highest and lowest salary\n",
    "4. Any interesting patterns\n",
    "\"\"\"\n",
    "\n",
    "reply, session_id = client.chat_new(\n",
    "    model=MODEL,\n",
    "    user_message=query,\n",
    "    agent_type=\"auto\",\n",
    "    files=[test_csv_path]\n",
    ")\n",
    "\n",
    "print(\"\\n=== AI Response ===\")\n",
    "print(reply)\n",
    "print(f\"\\nSession ID: {session_id}\")\n",
    "\n",
    "# Cleanup\n",
    "Path(test_csv_path).unlink()\n",
    "print(f\"\\n✓ Cleaned up test file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 19) File Upload with Chat - CSV Data Analysis\n",
    "\n",
    "Test the new file upload feature by creating and analyzing a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18) Python Code Generation - Excel File Analysis (with File Upload)\n",
    "# Now using the new file attachment feature!\n",
    "\n",
    "excel_path = f\"data/uploads/{username}/폴드긍정.xlsx\"\n",
    "\n",
    "excel_analysis_query = \"\"\"\n",
    "Analyze the attached Excel file and:\n",
    "1. What's the most widely appreciated feature?\n",
    "2. What's the most widely used phrase?\n",
    "3. What's the least used phrase?\n",
    "\n",
    "Make sure to handle Korean text encoding properly.\n",
    "\"\"\"\n",
    "\n",
    "# NEW: Upload the file with the chat request\n",
    "excel_reply, _ = client.chat_new(\n",
    "    MODEL, \n",
    "    excel_analysis_query, \n",
    "    agent_type=\"auto\",\n",
    "    files=[excel_path]  # Attach the file!\n",
    ")\n",
    "\n",
    "print(\"Excel File Analysis Response:\")\n",
    "print(excel_reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20. 끝말잇기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '끝말 잇기하자 나부터 시작할게: 이시훈'\n",
    "\n",
    "reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
