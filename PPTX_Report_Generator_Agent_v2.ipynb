{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-Orchestrated Report Generator (using Agentic LLM API developed by SiHun Lee, CAE G., MX div., SEC.)\n",
    "\n",
    "This notebook uses **LLM API's agentic capabilities** to automatically generate comprehensive PDF and PowerPoint reports from warpage data.\n",
    "\n",
    "## How It Works\n",
    "\n",
    "1. **Phase 1:** Analyze data → finds outliers, calculates statistics\n",
    "2. **Phase 2:** Generate charts → uses Phase 1 findings (not raw files)\n",
    "3. **Phase 3:** Build PDF Report → comprehensive, beautiful PDF document\n",
    "4. **Phase 4:** Build PowerPoint → comprehensive presentation with same content\n",
    "\n",
    "**Key Advantage:** Each phase reuses conversation memory, avoiding redundant file processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import time\n",
    "from IPython.display import display, Latex\n",
    "\n",
    "# Build Universal LLM API Client\n",
    "class LLMApiClient:\n",
    "    def __init__(self, base_url: str, timeout: float = 360000.0):\n",
    "        self.base_url = base_url.rstrip(\"/\")\n",
    "        self.token = None\n",
    "        self.timeout = httpx.Timeout(50.0, read=timeout, write=timeout, pool=timeout)\n",
    "\n",
    "    def _headers(self):\n",
    "        return {\"Authorization\": f\"Bearer {self.token}\"} if self.token else {}\n",
    "\n",
    "    def login(self, username: str, password: str):\n",
    "        r = httpx.post(f\"{self.base_url}/api/auth/login\", \n",
    "                      json={\"username\": username, \"password\": password}, timeout=10.0)\n",
    "        r.raise_for_status()\n",
    "        self.token = r.json()[\"access_token\"]\n",
    "        return r.json()\n",
    "\n",
    "    def list_models(self):\n",
    "        r = httpx.get(f\"{self.base_url}/v1/models\", headers=self._headers(), timeout=10.0)\n",
    "        r.raise_for_status()\n",
    "        return r.json()\n",
    "\n",
    "    def chat_new(self, model: str, user_message: str, agent_type: str = \"auto\", files: list = None):\n",
    "        messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "        data = {\"model\": model, \"messages\": json.dumps(messages), \"agent_type\": agent_type}\n",
    "        \n",
    "        files_to_upload = []\n",
    "        if files:\n",
    "            for file_path in files:\n",
    "                f = open(file_path, \"rb\")\n",
    "                files_to_upload.append((\"files\", (Path(file_path).name, f)))\n",
    "        \n",
    "        try:\n",
    "            r = httpx.post(f\"{self.base_url}/v1/chat/completions\", data=data,\n",
    "                          files=files_to_upload if files_to_upload else None,\n",
    "                          headers=self._headers(), timeout=self.timeout)\n",
    "            r.raise_for_status()\n",
    "            result = r.json()\n",
    "            return result[\"choices\"][0][\"message\"][\"content\"], result[\"x_session_id\"]\n",
    "        finally:\n",
    "            for _, (_, f) in files_to_upload:\n",
    "                f.close()\n",
    "\n",
    "    def chat_continue(self, model: str, session_id: str, user_message: str, \n",
    "                     agent_type: str = \"auto\", files: list = None):\n",
    "        messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "        data = {\"model\": model, \"messages\": json.dumps(messages), \n",
    "                \"session_id\": session_id, \"agent_type\": agent_type}\n",
    "        \n",
    "        files_to_upload = []\n",
    "        if files:\n",
    "            for file_path in files:\n",
    "                f = open(file_path, \"rb\")\n",
    "                files_to_upload.append((\"files\", (Path(file_path).name, f)))\n",
    "        \n",
    "        try:\n",
    "            r = httpx.post(f\"{self.base_url}/v1/chat/completions\", data=data,\n",
    "                          files=files_to_upload if files_to_upload else None,\n",
    "                          headers=self._headers(), timeout=self.timeout)\n",
    "            r.raise_for_status()\n",
    "            result = r.json()\n",
    "            return result[\"choices\"][0][\"message\"][\"content\"], result[\"x_session_id\"]\n",
    "        finally:\n",
    "            for _, (_, f) in files_to_upload:\n",
    "                f.close()\n",
    "\n",
    "    def get_session_artifacts(self, session_id: str):\n",
    "        \"\"\"Get list of files generated during the session\"\"\"\n",
    "        r = httpx.get(f\"{self.base_url}/api/chat/sessions/{session_id}/artifacts\",\n",
    "                     headers=self._headers(), timeout=10.0)\n",
    "        r.raise_for_status()\n",
    "        return r.json()\n",
    "\n",
    "    def download_artifact(self, session_id: str, filename: str, save_to: str = None):\n",
    "        \"\"\"\n",
    "        Download a generated artifact file to local disk.\n",
    "        \n",
    "        Args:\n",
    "            session_id: The session ID that generated the file\n",
    "            filename: Name of the file to download (can include subdirectory, e.g., 'temp_charts/chart.png')\n",
    "            save_to: Local path to save the file (default: current directory with original filename)\n",
    "        \n",
    "        Returns:\n",
    "            str: Path to the downloaded file\n",
    "        \n",
    "        Example:\n",
    "            client.download_artifact(session_id, \"Warpage_Report_20250126.pptx\", \"./downloads/report.pptx\")\n",
    "        \"\"\"\n",
    "        r = httpx.get(\n",
    "            f\"{self.base_url}/api/chat/sessions/{session_id}/artifacts/{filename}\",\n",
    "            headers=self._headers(),\n",
    "            timeout=60.0\n",
    "        )\n",
    "        r.raise_for_status()\n",
    "        \n",
    "        # Determine local save path\n",
    "        if save_to is None:\n",
    "            save_to = Path(filename).name  # Use just the filename, not subdirectory\n",
    "        \n",
    "        # Create parent directories if needed\n",
    "        save_path = Path(save_to)\n",
    "        save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Write file content\n",
    "        with open(save_path, \"wb\") as f:\n",
    "            f.write(r.content)\n",
    "        \n",
    "        return str(save_path)\n",
    "\n",
    "# Configuration\n",
    "API_BASE_URL = 'http://localhost:10007'\n",
    "USERNAME = \"leesihun\"\n",
    "PASSWORD = \"s.hun.lee\"\n",
    "\n",
    "# Initialize and login\n",
    "client = LLMApiClient(API_BASE_URL, timeout=36000.0)# 10 hours\n",
    "client.login(USERNAME, PASSWORD)\n",
    "models = client.list_models()\n",
    "MODEL = models[\"data\"][0][\"id\"]\n",
    "\n",
    "print(f\"✓ Logged in as: {USERNAME}\")\n",
    "print(f\"✓ Using model: {MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your data files\n",
    "stats_paths = [\n",
    "    Path(\"B8_1021_stats.json\"),\n",
    "    Path(\"B8_1027_stats.json\"),\n",
    "]\n",
    "\n",
    "# Verify files exist\n",
    "print(f\"Configured {len(stats_paths)} data file(s):\\n\")\n",
    "for i, path in enumerate(stats_paths, 1):\n",
    "    if path.exists():\n",
    "        size_kb = path.stat().st_size / 1024\n",
    "        print(f\"  [{i}] {path.name} ({size_kb:.1f} KB) - ✓\")\n",
    "    else:\n",
    "        print(f\"  [{i}] {path.name} - ✗ NOT FOUND\")\n",
    "\n",
    "file_paths_str = [str(p) for p in stats_paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Phase 1: Data Analysis\n",
    "\n",
    "The AI will analyze your data and identify key patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_prompt = f\"\"\"\n",
    "Analyze {len(stats_paths)} warpage measurement JSON files attached.\n",
    "\n",
    "Input Data Structure:\n",
    "- Each file contain warpage statistics per PCB board\n",
    "- Statistics: min, max, range (warpage value), mean, median, std, skewness, kurtosis\n",
    "- PCA values (pc1, pc2) calculated within each source_pdf\n",
    "- Filenames contain acquisition date/time (e.g., 1021 = October 21th)\n",
    "- Note that usually, mean, median is not important. To assess warpage, range is the single most important feature.\n",
    "\n",
    "Tasks:\n",
    "1. Calculate overall statistics (mean, std, min, max of range across all files)\n",
    "2. Identify PCA-based outliers using pc1, pc2 values. Look for PCA values that are quite a far from others\n",
    "3. Compare production dates - which is better quality and why?\n",
    "4. List specific outlier filenames with reasons\n",
    "5. Save your results to a numpy array locally\n",
    "\n",
    "**Required Output:**\n",
    "- Total measurements count\n",
    "- Outlier list with full filenames\n",
    "- Production date comparison (winner + reason)\n",
    "- Key concerns or patterns\n",
    "\n",
    "Think HARD!\n",
    "\"\"\"\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE 1: DATA ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "start = time.time()\n",
    "analysis_result, session_id = client.chat_new(\n",
    "    MODEL, analysis_prompt, agent_type=\"auto\", files=file_paths_str\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Analysis completed in {time.time() - start:.1f}s\\n\")\n",
    "print(\"=\" * 80)\n",
    "display(Latex(analysis_result))\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Phase 2: Generate Visualizations\n",
    "\n",
    "**Key:** AI reuses Phase 1 findings from conversation memory (not raw files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_prompt = f\"\"\"\n",
    "**PRIORITY: Use your Phase 1 analysis from conversation memory and saved numpy array.**\n",
    "\n",
    "In Phase 1, you already:\n",
    "- Analyzed {len(stats_paths)} datasets and loaded all data\n",
    "- Identified PCA outliers with pc1, pc2 values\n",
    "- Compared production dates\n",
    "- Listed specific outlier filenames\n",
    "\n",
    "**Avoid re-analyze raw files if possible. Use your Phase 1 findings and file.**\n",
    "Files attached are ONLY for verification if needed.\n",
    "\n",
    "**Task:** Create visualizations and classify outliers\n",
    "\n",
    "**Outlier Classification:**\n",
    "- **BAD outliers:** High mean/std/range (critical quality issues)\n",
    "- **GOOD outliers:** Unusual PCA position but acceptable metrics\n",
    "- **Normal:** Within PCA cluster, standard metrics\n",
    "\n",
    "**Required Charts** (save to temp_charts/):\n",
    "1. `pca_outliers_classified.png` - PC1 vs PC2 scatter (Blue=normal, Orange=good outlier, RED=bad outlier)\n",
    "2. `bad_outliers_detail.png` - Bar chart comparing bad outliers vs average\n",
    "3. `production_comparison.png` - Production date quality comparison\n",
    "4. Additional charts as appropriate (distributions, trends, control charts, etc.)\n",
    "\n",
    "**Style:** 300 DPI, seaborn whitegrid, professional colors\n",
    "\n",
    "**Required Output:**\n",
    "- List of generated chart files\n",
    "- Bad outlier summary (file IDs + reasons)\n",
    "- Production date insights\n",
    "\n",
    "THINK HARD!\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE 2: VISUALIZATION GENERATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "start = time.time()\n",
    "viz_result, _ = client.chat_continue(\n",
    "    MODEL, session_id, viz_prompt, agent_type=\"auto\", files=file_paths_str\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Visualizations completed in {time.time() - start:.1f}s\\n\")\n",
    "print(\"=\" * 80)\n",
    "display(Latex(viz_result))\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Phase 3: PDF Report Assembly\n",
    "\n",
    "**Key:** AI uses Phase 1 & 2 findings from conversation memory to create a beautiful, comprehensive PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get total file count\ntotal_files = 0\nfor path in stats_paths:\n    with open(path, 'r') as f:\n        data = json.load(f)\n        total_files += len(data.get('files', []))\n\n# Phase 3: PDF Report Generation\npdf_prompt = f\"\"\"\n**PRIORITY: Use Phase 1 & 2 findings from conversation memory and files.**\n\nYou have:\n- Phase 1: Statistics, outlier IDs, production date comparison\n- Phase 2: Images useful for report generation, bad outlier classifications\n\n**Avoid re-analyze raw files if possible. Use conversation context.**\nFiles attached are ONLY for verification if needed.\n\n**Task:** Create comprehensive, beautiful PDF report using ReportLab library.\n\n**CRITICAL: Use the following structured template approach to ensure consistent formatting**\n\n## Installation\n```python\n# Ensure ReportLab is installed\nimport subprocess\nsubprocess.run(['pip', 'install', 'reportlab'], check=False)\n```\n\n## PDF Configuration (MUST USE THESE EXACT SETTINGS)\n\n```python\nfrom reportlab.lib.pagesizes import A4\nfrom reportlab.lib.units import cm\nfrom reportlab.lib import colors\nfrom reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, PageBreak, Image, Table, TableStyle\nfrom reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\nfrom reportlab.lib.enums import TA_CENTER, TA_LEFT, TA_JUSTIFY\nfrom datetime import datetime\nfrom pathlib import Path\n\n# Page setup - CONSISTENT A4 PORTRAIT ONLY\nPAGE_WIDTH, PAGE_HEIGHT = A4  # 595.27 x 841.89 points (210mm x 297mm)\nMARGIN = 2.5 * cm  # 25mm margins on all sides\nCONTENT_WIDTH = PAGE_WIDTH - 2 * MARGIN  # ~16cm usable width\n\n# Image sizing - CONSISTENT across ALL pages\nIMAGE_MAX_WIDTH = CONTENT_WIDTH  # Full content width\nIMAGE_MAX_HEIGHT = 12 * cm  # 12cm max height for consistency\n```\n\n## Helper Functions (MUST IMPLEMENT EXACTLY AS SHOWN)\n\n```python\ndef resize_image_to_fit(image_path, max_width, max_height):\n    \\\"\\\"\\\"\n    Resize image maintaining aspect ratio to fit within max dimensions.\n    Centers the image horizontally.\n    \\\"\\\"\\\"\n    from reportlab.platypus import Image\n    \n    img = Image(image_path)\n    img_width = img.imageWidth\n    img_height = img.imageHeight\n    aspect = img_height / img_width\n    \n    # Calculate target dimensions\n    target_width = max_width\n    target_height = target_width * aspect\n    \n    # If too tall, scale down by height\n    if target_height > max_height:\n        target_height = max_height\n        target_width = target_height / aspect\n    \n    # Set final dimensions\n    img.drawWidth = target_width\n    img.drawHeight = target_height\n    img.hAlign = 'CENTER'  # Center horizontally\n    \n    return img\n\ndef add_header_footer(canvas, doc):\n    \\\"\\\"\\\"Add header and footer to all pages except cover page\\\"\\\"\\\"\n    canvas.saveState()\n    \n    if doc.page > 1:  # Skip cover page\n        # Header\n        canvas.setFont('Helvetica', 9)\n        canvas.setFillColor(colors.grey)\n        canvas.drawString(MARGIN, PAGE_HEIGHT - 1.5*cm, \"Warpage Analysis Report\")\n        \n        # Footer with page number\n        canvas.setFont('Helvetica', 9)\n        page_num_text = f\"Page {{doc.page - 1}}\"  # Exclude cover from count\n        canvas.drawCentredString(PAGE_WIDTH / 2, 1.5*cm, page_num_text)\n    \n    canvas.restoreState()\n```\n\n## Style Definitions (USE THESE EXACT STYLES)\n\n```python\nstyles = getSampleStyleSheet()\n\n# Custom styles for consistency\nstyle_title = ParagraphStyle(\n    'CustomTitle',\n    parent=styles['Title'],\n    fontSize=24,\n    textColor=colors.HexColor('#1f4788'),\n    alignment=TA_CENTER,\n    spaceAfter=12,\n    leading=30\n)\n\nstyle_subtitle = ParagraphStyle(\n    'CustomSubtitle',\n    parent=styles['Normal'],\n    fontSize=14,\n    textColor=colors.HexColor('#1f4788'),\n    alignment=TA_CENTER,\n    spaceAfter=6\n)\n\nstyle_heading1 = ParagraphStyle(\n    'CustomHeading1',\n    parent=styles['Heading1'],\n    fontSize=18,\n    textColor=colors.HexColor('#1f4788'),\n    spaceAfter=12,\n    spaceBefore=0\n)\n\nstyle_heading2 = ParagraphStyle(\n    'CustomHeading2',\n    parent=styles['Heading2'],\n    fontSize=14,\n    textColor=colors.HexColor('#1f4788'),\n    spaceAfter=8,\n    spaceBefore=0\n)\n\nstyle_body = ParagraphStyle(\n    'CustomBody',\n    parent=styles['BodyText'],\n    fontSize=11,\n    alignment=TA_JUSTIFY,\n    spaceAfter=8,\n    leading=14\n)\n\nstyle_bullet = ParagraphStyle(\n    'CustomBullet',\n    parent=styles['BodyText'],\n    fontSize=11,\n    leftIndent=20,\n    spaceAfter=6,\n    leading=14,\n    bulletIndent=10\n)\n```\n\n## Document Structure (FOLLOW THIS EXACT ORDER)\n\n```python\n# Initialize document\nfilename = f'Warpage_Report_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.pdf'\ndoc = SimpleDocTemplate(\n    filename,\n    pagesize=A4,\n    leftMargin=MARGIN,\n    rightMargin=MARGIN,\n    topMargin=MARGIN,\n    bottomMargin=MARGIN\n)\n\n# Story list - holds all content\nstory = []\n\n# === PAGE 1: COVER PAGE ===\nstory.append(Spacer(1, 6*cm))\nstory.append(Paragraph(\"Automatic Warpage Analysis Report\", style_title))\nstory.append(Spacer(1, 1*cm))\nstory.append(Paragraph(f\"Analysis of {total_files} Measurements ({len(stats_paths)} Production Dates)\", style_subtitle))\nstory.append(Spacer(1, 0.5*cm))\nstory.append(Paragraph(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M')}\", style_body))\nstory.append(PageBreak())\n\n# === PAGE 2: TABLE OF CONTENTS ===\nstory.append(Paragraph(\"Table of Contents\", style_heading1))\nstory.append(Spacer(1, 0.5*cm))\n\ntoc_data = [\n    [\"Section\", \"Page\"],\n    [\"Executive Summary\", \"3\"],\n    [\"PCA Outlier Classification\", \"4\"],\n    [\"Bad Outlier Details\", \"5\"],\n    [\"Production Comparison\", \"6\"],\n    [\"Additional Analysis\", \"7+\"],\n    [\"Recommendations\", \"Last\"]\n]\n\ntoc_table = Table(toc_data, colWidths=[CONTENT_WIDTH * 0.7, CONTENT_WIDTH * 0.3])\ntoc_table.setStyle(TableStyle([\n    ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor('#1f4788')),\n    ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),\n    ('ALIGN', (0, 0), (-1, -1), 'LEFT'),\n    ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n    ('FONTSIZE', (0, 0), (-1, 0), 12),\n    ('BOTTOMPADDING', (0, 0), (-1, 0), 12),\n    ('GRID', (0, 0), (-1, -1), 0.5, colors.grey),\n    ('FONTNAME', (0, 1), (-1, -1), 'Helvetica'),\n    ('FONTSIZE', (0, 1), (-1, -1), 11),\n    ('ROWBACKGROUNDS', (0, 1), (-1, -1), [colors.white, colors.HexColor('#f0f0f0')])\n]))\nstory.append(toc_table)\nstory.append(PageBreak())\n\n# === PAGE 3: EXECUTIVE SUMMARY ===\nstory.append(Paragraph(\"Executive Summary\", style_heading1))\nstory.append(Spacer(1, 0.5*cm))\nstory.append(Paragraph(\"Key findings from the warpage analysis:\", style_body))\nstory.append(Spacer(1, 0.3*cm))\n\n# Add bullet points summarizing Phase 1 & 2 findings\nsummary_points = [\n    f\"Total measurements analyzed: {total_files}\",\n    \"PCA-based outlier detection identified critical quality issues\",\n    \"Production date comparison reveals significant quality variations\",\n    \"Bad outliers show elevated range, mean, and standard deviation\",\n]\n\nfor point in summary_points:\n    story.append(Paragraph(f\"• {point}\", style_bullet))\n\nstory.append(PageBreak())\n\n# === PAGE 4: PCA OUTLIER CLASSIFICATION ===\nstory.append(Paragraph(\"PCA Outlier Classification\", style_heading2))\nstory.append(Spacer(1, 0.3*cm))\nstory.append(Paragraph(\n    \"The scatter plot below shows PC1 vs PC2 values with outliers classified into three categories: \"\n    \"normal (blue), good outliers with unusual PCA position but acceptable metrics (orange), \"\n    \"and bad outliers with critical quality issues (red).\",\n    style_body\n))\nstory.append(Spacer(1, 0.5*cm))\n\nimg = resize_image_to_fit(\"temp_charts/pca_outliers_classified.png\", IMAGE_MAX_WIDTH, IMAGE_MAX_HEIGHT)\nstory.append(img)\nstory.append(PageBreak())\n\n# === PAGE 5: BAD OUTLIER DETAILS ===\nstory.append(Paragraph(\"Bad Outlier Details\", style_heading2))\nstory.append(Spacer(1, 0.3*cm))\nstory.append(Paragraph(\n    \"This chart compares bad outliers against average metrics, showing elevated range, \"\n    \"mean, and standard deviation values that indicate critical warpage issues.\",\n    style_body\n))\nstory.append(Spacer(1, 0.5*cm))\n\nimg = resize_image_to_fit(\"temp_charts/bad_outliers_detail.png\", IMAGE_MAX_WIDTH, IMAGE_MAX_HEIGHT)\nstory.append(img)\nstory.append(PageBreak())\n\n# === PAGE 6: PRODUCTION COMPARISON ===\nstory.append(Paragraph(\"Production Comparison\", style_heading2))\nstory.append(Spacer(1, 0.3*cm))\nstory.append(Paragraph(\n    \"Quality comparison between production dates reveals which batch demonstrated better performance \"\n    \"and lower warpage variability.\",\n    style_body\n))\nstory.append(Spacer(1, 0.5*cm))\n\nimg = resize_image_to_fit(\"temp_charts/production_comparison.png\", IMAGE_MAX_WIDTH, IMAGE_MAX_HEIGHT)\nstory.append(img)\nstory.append(PageBreak())\n\n# === PAGES 7+: ADDITIONAL CHARTS ===\n# Find all charts in temp_charts/ directory\nimport glob\nall_charts = glob.glob(\"temp_charts/*.png\")\nrequired_charts = [\"pca_outliers_classified.png\", \"bad_outliers_detail.png\", \"production_comparison.png\"]\n\nfor chart_path in sorted(all_charts):\n    chart_filename = Path(chart_path).name\n    \n    # Skip already included charts\n    if chart_filename in required_charts:\n        continue\n    \n    # Generate title from filename\n    chart_title = chart_filename.replace(\"_\", \" \").replace(\".png\", \"\").title()\n    \n    story.append(Paragraph(chart_title, style_heading2))\n    story.append(Spacer(1, 0.3*cm))\n    story.append(Paragraph(f\"Additional analysis: {chart_title}\", style_body))\n    story.append(Spacer(1, 0.5*cm))\n    \n    img = resize_image_to_fit(chart_path, IMAGE_MAX_WIDTH, IMAGE_MAX_HEIGHT)\n    story.append(img)\n    story.append(PageBreak())\n\n# === LAST PAGE: RECOMMENDATIONS ===\nstory.append(Paragraph(\"Recommendations\", style_heading1))\nstory.append(Spacer(1, 0.5*cm))\nstory.append(Paragraph(\"Based on the analysis, the following actions are recommended:\", style_body))\nstory.append(Spacer(1, 0.3*cm))\n\nrecommendations = [\n    \"Investigate root causes of bad outliers identified in PCA analysis\",\n    \"Implement stricter quality controls for production batches with higher warpage\",\n    \"Focus process improvements on reducing range variability\",\n    \"Consider real-time PCA monitoring for early outlier detection\",\n    \"Review and optimize manufacturing parameters for problematic production dates\"\n]\n\nfor rec in recommendations:\n    story.append(Paragraph(f\"• {rec}\", style_bullet))\n\n# === BUILD PDF ===\ndoc.build(story, onFirstPage=add_header_footer, onLaterPages=add_header_footer)\n\nprint(f\"✓ PDF generated: {filename}\")\n```\n\n## Critical Requirements\n\n**MUST follow these rules:**\n1. **Portrait orientation ONLY** - Never change page orientation\n2. **Consistent image sizing** - Always use `resize_image_to_fit()` with IMAGE_MAX_WIDTH and IMAGE_MAX_HEIGHT\n3. **One section per page** - Always add `PageBreak()` after each section\n4. **Use defined styles** - Only use the custom styles defined above\n5. **Maintain aspect ratios** - Images must scale proportionally\n6. **Center images** - Use `img.hAlign = 'CENTER'`\n7. **Consistent spacing** - Use Spacer elements as shown\n\n## Expected Output\n\nFilename: `Warpage_Report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf`\n\n**Verify before completing:**\n- [ ] All images are same width and properly centered\n- [ ] All pages are portrait orientation\n- [ ] Each section starts on a new page\n- [ ] Headers/footers appear on all pages except cover\n- [ ] TOC is properly formatted\n- [ ] Professional color scheme (#1f4788 for headings)\n\nIMPLEMENT THIS EXACTLY AS SPECIFIED!\n\"\"\"\n\nprint(\"=\" * 80)\nprint(\"PHASE 3: PDF REPORT ASSEMBLY\")\nprint(\"=\" * 80)\n\nstart = time.time()\npdf_result, _ = client.chat_continue(\n    MODEL, session_id, pdf_prompt, agent_type=\"auto\", files=file_paths_str\n)\n\nprint(f\"\\n✓ PDF report completed in {time.time() - start:.1f}s\\n\")\nprint(\"=\" * 80)\ndisplay(Latex(pdf_result))\nprint(\"=\" * 80)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}