{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-Orchestrated Report Generator (using Agentic LLM API developed by SiHun Lee, CAE G., MX div., SEC.)\n",
    "\n",
    "This notebook uses **LLM API's agentic capabilities** to automatically generate comprehensive PDF and PowerPoint reports from warpage data.\n",
    "\n",
    "## How It Works\n",
    "\n",
    "1. **Phase 1:** Analyze data → finds outliers, calculates statistics\n",
    "2. **Phase 2:** Generate charts → uses Phase 1 findings (not raw files)\n",
    "3. **Phase 3:** Build PDF Report → comprehensive, beautiful PDF document\n",
    "4. **Phase 4:** Build PowerPoint → comprehensive presentation with same content\n",
    "\n",
    "**Key Advantage:** Each phase reuses conversation memory, avoiding redundant file processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import time\n",
    "from IPython.display import display, Latex\n",
    "\n",
    "# Build Universal LLM API Client\n",
    "class LLMApiClient:\n",
    "    def __init__(self, base_url: str, timeout: float = 360000.0):\n",
    "        self.base_url = base_url.rstrip(\"/\")\n",
    "        self.token = None\n",
    "        self.timeout = httpx.Timeout(50.0, read=timeout, write=timeout, pool=timeout)\n",
    "\n",
    "    def _headers(self):\n",
    "        return {\"Authorization\": f\"Bearer {self.token}\"} if self.token else {}\n",
    "\n",
    "    def login(self, username: str, password: str):\n",
    "        r = httpx.post(f\"{self.base_url}/api/auth/login\", \n",
    "                      json={\"username\": username, \"password\": password}, timeout=10.0)\n",
    "        r.raise_for_status()\n",
    "        self.token = r.json()[\"access_token\"]\n",
    "        return r.json()\n",
    "\n",
    "    def list_models(self):\n",
    "        r = httpx.get(f\"{self.base_url}/v1/models\", headers=self._headers(), timeout=10.0)\n",
    "        r.raise_for_status()\n",
    "        return r.json()\n",
    "\n",
    "    def chat_new(self, model: str, user_message: str, agent_type: str = \"auto\", files: list = None):\n",
    "        messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "        data = {\"model\": model, \"messages\": json.dumps(messages), \"agent_type\": agent_type}\n",
    "        \n",
    "        files_to_upload = []\n",
    "        if files:\n",
    "            for file_path in files:\n",
    "                f = open(file_path, \"rb\")\n",
    "                files_to_upload.append((\"files\", (Path(file_path).name, f)))\n",
    "        \n",
    "        try:\n",
    "            r = httpx.post(f\"{self.base_url}/v1/chat/completions\", data=data,\n",
    "                          files=files_to_upload if files_to_upload else None,\n",
    "                          headers=self._headers(), timeout=self.timeout)\n",
    "            r.raise_for_status()\n",
    "            result = r.json()\n",
    "            return result[\"choices\"][0][\"message\"][\"content\"], result[\"x_session_id\"]\n",
    "        finally:\n",
    "            for _, (_, f) in files_to_upload:\n",
    "                f.close()\n",
    "\n",
    "    def chat_continue(self, model: str, session_id: str, user_message: str, \n",
    "                     agent_type: str = \"auto\", files: list = None):\n",
    "        messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "        data = {\"model\": model, \"messages\": json.dumps(messages), \n",
    "                \"session_id\": session_id, \"agent_type\": agent_type}\n",
    "        \n",
    "        files_to_upload = []\n",
    "        if files:\n",
    "            for file_path in files:\n",
    "                f = open(file_path, \"rb\")\n",
    "                files_to_upload.append((\"files\", (Path(file_path).name, f)))\n",
    "        \n",
    "        try:\n",
    "            r = httpx.post(f\"{self.base_url}/v1/chat/completions\", data=data,\n",
    "                          files=files_to_upload if files_to_upload else None,\n",
    "                          headers=self._headers(), timeout=self.timeout)\n",
    "            r.raise_for_status()\n",
    "            result = r.json()\n",
    "            return result[\"choices\"][0][\"message\"][\"content\"], result[\"x_session_id\"]\n",
    "        finally:\n",
    "            for _, (_, f) in files_to_upload:\n",
    "                f.close()\n",
    "\n",
    "    def get_session_artifacts(self, session_id: str):\n",
    "        \"\"\"Get list of files generated during the session\"\"\"\n",
    "        r = httpx.get(f\"{self.base_url}/api/chat/sessions/{session_id}/artifacts\",\n",
    "                     headers=self._headers(), timeout=10.0)\n",
    "        r.raise_for_status()\n",
    "        return r.json()\n",
    "\n",
    "    def download_artifact(self, session_id: str, filename: str, save_to: str = None):\n",
    "        \"\"\"\n",
    "        Download a generated artifact file to local disk.\n",
    "        \n",
    "        Args:\n",
    "            session_id: The session ID that generated the file\n",
    "            filename: Name of the file to download (can include subdirectory, e.g., 'temp_charts/chart.png')\n",
    "            save_to: Local path to save the file (default: current directory with original filename)\n",
    "        \n",
    "        Returns:\n",
    "            str: Path to the downloaded file\n",
    "        \n",
    "        Example:\n",
    "            client.download_artifact(session_id, \"Warpage_Report_20250126.pptx\", \"./downloads/report.pptx\")\n",
    "        \"\"\"\n",
    "        r = httpx.get(\n",
    "            f\"{self.base_url}/api/chat/sessions/{session_id}/artifacts/{filename}\",\n",
    "            headers=self._headers(),\n",
    "            timeout=60.0\n",
    "        )\n",
    "        r.raise_for_status()\n",
    "        \n",
    "        # Determine local save path\n",
    "        if save_to is None:\n",
    "            save_to = Path(filename).name  # Use just the filename, not subdirectory\n",
    "        \n",
    "        # Create parent directories if needed\n",
    "        save_path = Path(save_to)\n",
    "        save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Write file content\n",
    "        with open(save_path, \"wb\") as f:\n",
    "            f.write(r.content)\n",
    "        \n",
    "        return str(save_path)\n",
    "\n",
    "# Configuration\n",
    "API_BASE_URL = 'http://localhost:10007'\n",
    "USERNAME = \"leesihun\"\n",
    "PASSWORD = \"s.hun.lee\"\n",
    "\n",
    "# Initialize and login\n",
    "client = LLMApiClient(API_BASE_URL, timeout=36000.0)# 10 hours\n",
    "client.login(USERNAME, PASSWORD)\n",
    "models = client.list_models()\n",
    "MODEL = models[\"data\"][0][\"id\"]\n",
    "\n",
    "print(f\"✓ Logged in as: {USERNAME}\")\n",
    "print(f\"✓ Using model: {MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your data files\n",
    "stats_paths = [\n",
    "    Path(\"B8_1021_stats.json\"),\n",
    "    Path(\"B8_1027_stats.json\"),\n",
    "]\n",
    "\n",
    "# Verify files exist\n",
    "print(f\"Configured {len(stats_paths)} data file(s):\\n\")\n",
    "for i, path in enumerate(stats_paths, 1):\n",
    "    if path.exists():\n",
    "        size_kb = path.stat().st_size / 1024\n",
    "        print(f\"  [{i}] {path.name} ({size_kb:.1f} KB) - ✓\")\n",
    "    else:\n",
    "        print(f\"  [{i}] {path.name} - ✗ NOT FOUND\")\n",
    "\n",
    "file_paths_str = [str(p) for p in stats_paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Phase 1: Data Analysis\n",
    "\n",
    "The AI will analyze your data and identify key patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_prompt = f\"\"\"\n",
    "Analyze {len(stats_paths)} warpage measurement JSON files attached.\n",
    "\n",
    "Input Data Structure:\n",
    "- Each file contain warpage statistics per PCB board\n",
    "- Statistics: min, max, range (warpage value), mean, median, std, skewness, kurtosis\n",
    "- PCA values (pc1, pc2) calculated within each source_pdf\n",
    "- Filenames contain acquisition date/time (e.g., 1021 = October 21th)\n",
    "- Note that usually, mean, median is not important. To assess warpage, range is the single most important feature.\n",
    "\n",
    "Tasks:\n",
    "1. Calculate overall statistics (mean, std, min, max of range across all files)\n",
    "2. Identify PCA-based outliers using pc1, pc2 values. Look for PCA values that are quite a far from others\n",
    "3. Compare production dates - which is better quality and why?\n",
    "4. List specific outlier filenames with reasons\n",
    "5. Save your results to a numpy array locally\n",
    "\n",
    "**Required Output:**\n",
    "- Total measurements count\n",
    "- Outlier list with full filenames\n",
    "- Production date comparison (winner + reason)\n",
    "- Key concerns or patterns\n",
    "\n",
    "Think HARD!\n",
    "\"\"\"\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE 1: DATA ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "start = time.time()\n",
    "analysis_result, session_id = client.chat_new(\n",
    "    MODEL, analysis_prompt, agent_type=\"auto\", files=file_paths_str\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Analysis completed in {time.time() - start:.1f}s\\n\")\n",
    "print(\"=\" * 80)\n",
    "display(Latex(analysis_result))\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Phase 2: Generate Visualizations\n",
    "\n",
    "**Key:** AI reuses Phase 1 findings from conversation memory (not raw files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_prompt = f\"\"\"\n",
    "**PRIORITY: Use your Phase 1 analysis from conversation memory and saved numpy array.**\n",
    "\n",
    "In Phase 1, you already:\n",
    "- Analyzed {len(stats_paths)} datasets and loaded all data\n",
    "- Identified PCA outliers with pc1, pc2 values\n",
    "- Compared production dates\n",
    "- Listed specific outlier filenames\n",
    "\n",
    "**Avoid re-analyze raw files if possible. Use your Phase 1 findings and file.**\n",
    "Files attached are ONLY for verification if needed.\n",
    "\n",
    "**Task:** Create visualizations and classify outliers\n",
    "\n",
    "**Outlier Classification:**\n",
    "- **BAD outliers:** High mean/std/range (critical quality issues)\n",
    "- **GOOD outliers:** Unusual PCA position but acceptable metrics\n",
    "- **Normal:** Within PCA cluster, standard metrics\n",
    "\n",
    "**Required Charts** (save to temp_charts/):\n",
    "1. `pca_outliers_classified.png` - PC1 vs PC2 scatter (Blue=normal, Orange=good outlier, RED=bad outlier)\n",
    "2. `bad_outliers_detail.png` - Bar chart comparing bad outliers vs average\n",
    "3. `production_comparison.png` - Production date quality comparison\n",
    "4. Additional charts as appropriate (distributions, trends, control charts, etc.)\n",
    "\n",
    "**Style:** 300 DPI, seaborn whitegrid, professional colors\n",
    "\n",
    "**Required Output:**\n",
    "- List of generated chart files\n",
    "- Bad outlier summary (file IDs + reasons)\n",
    "- Production date insights\n",
    "\n",
    "THINK HARD!\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE 2: VISUALIZATION GENERATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "start = time.time()\n",
    "viz_result, _ = client.chat_continue(\n",
    "    MODEL, session_id, viz_prompt, agent_type=\"auto\", files=file_paths_str\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Visualizations completed in {time.time() - start:.1f}s\\n\")\n",
    "print(\"=\" * 80)\n",
    "display(Latex(viz_result))\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Phase 3: PDF Report Assembly\n",
    "\n",
    "**Key:** AI uses Phase 1 & 2 findings from conversation memory to create a beautiful, comprehensive PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get total file count\n",
    "total_files = 0\n",
    "for path in stats_paths:\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        total_files += len(data.get('files', []))\n",
    "\n",
    "# Phase 3: PDF Report Generation\n",
    "pdf_prompt = f\"\"\"\n",
    "**PRIORITY: Use Phase 1 & 2 findings from conversation memory and files.**\n",
    "\n",
    "You have:\n",
    "- Phase 1: Statistics, outlier IDs, production date comparison\n",
    "- Phase 2: Images useful for report generation, bad outlier classifications\n",
    "\n",
    "**Avoid re-analyze raw files if possible. Use conversation context.**\n",
    "Files attached are ONLY for verification if needed.\n",
    "\n",
    "**Task:** Create comprehensive, beautiful PDF report for the attached files.\n",
    "\n",
    "**Report:**\n",
    "- Title: \"Automatic Warpage Analysis Report\n",
    "- Subtitle: \"Analysis of {total_files} Measurements ({len(stats_paths)} Production Dates)\"\n",
    "- Filename: `Warpage_Report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf`\n",
    "- Format: A4 size (or Letter), professional margins\n",
    "\n",
    "**Document Structure:**\n",
    "\n",
    "1. **Cover Page** - Report title, subtitle, generation date, professional styling\n",
    "\n",
    "2. **Executive Summary**\n",
    "\n",
    "3. **PCA Outlier Classification** - with image\n",
    "\n",
    "4. **Bad Outlier Details** - with image\n",
    "\n",
    "5. **Production Comparison** -with image\n",
    "\n",
    "6-N. **Additional Charts** - with images\n",
    "\n",
    "N+1. **Recommendations** \n",
    "\n",
    "**Style Requirements:**\n",
    "- Professional color scheme\n",
    "- High-quality image embedding (maintain 300 DPI quality)\n",
    "- Professional typography (use standard fonts: Helvetica, Times, Courier)\n",
    "- Proper page breaks between sections\n",
    "- Headers/footers with page numbers\n",
    "- Table of Contents with page numbers\n",
    "- Beautiful, publication-quality output\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE 3: PDF REPORT ASSEMBLY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "start = time.time()\n",
    "pdf_result, _ = client.chat_continue(\n",
    "    MODEL, session_id, pdf_prompt, agent_type=\"auto\", files=file_paths_str\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ PDF report completed in {time.time() - start:.1f}s\\n\")\n",
    "print(\"=\" * 80)\n",
    "display(Latex(pdf_result))\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
