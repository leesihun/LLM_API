{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-Orchestrated PowerPoint Report Generator (v2 - Simplified)\n",
    "\n",
    "This notebook uses **your LLM API's agentic capabilities** to automatically generate comprehensive PowerPoint reports from warpage data.\n",
    "\n",
    "## What's New in v2\n",
    "\n",
    "- **90% shorter prompts** - Simplified from ~500 lines to ~50 lines total\n",
    "- **Conversation context reuse** - AI remembers previous phase findings\n",
    "- **Files as fallback** - Data files only for verification, not re-processing\n",
    "- **Clearer structure** - Each phase builds on previous work\n",
    "\n",
    "## How It Works\n",
    "\n",
    "1. **Phase 1:** Analyze data → finds outliers, calculates statistics\n",
    "2. **Phase 2:** Generate charts → uses Phase 1 findings (not raw files)\n",
    "3. **Phase 3:** Build PowerPoint → uses Phase 1 & 2 findings (not raw files)\n",
    "\n",
    "**Key Advantage:** Each phase reuses conversation memory, avoiding redundant file processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import time\n",
    "from IPython.display import display, Latex\n",
    "\n",
    "# LLM API Client\n",
    "class LLMApiClient:\n",
    "    def __init__(self, base_url: str, timeout: float = 3600.0):\n",
    "        self.base_url = base_url.rstrip(\"/\")\n",
    "        self.token = None\n",
    "        self.timeout = httpx.Timeout(50.0, read=timeout, write=timeout, pool=timeout)\n",
    "\n",
    "    def _headers(self):\n",
    "        return {\"Authorization\": f\"Bearer {self.token}\"} if self.token else {}\n",
    "\n",
    "    def login(self, username: str, password: str):\n",
    "        r = httpx.post(f\"{self.base_url}/api/auth/login\", \n",
    "                      json={\"username\": username, \"password\": password}, timeout=10.0)\n",
    "        r.raise_for_status()\n",
    "        self.token = r.json()[\"access_token\"]\n",
    "        return r.json()\n",
    "\n",
    "    def list_models(self):\n",
    "        r = httpx.get(f\"{self.base_url}/v1/models\", headers=self._headers(), timeout=10.0)\n",
    "        r.raise_for_status()\n",
    "        return r.json()\n",
    "\n",
    "    def chat_new(self, model: str, user_message: str, agent_type: str = \"auto\", files: list = None):\n",
    "        messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "        data = {\"model\": model, \"messages\": json.dumps(messages), \"agent_type\": agent_type}\n",
    "        \n",
    "        files_to_upload = []\n",
    "        if files:\n",
    "            for file_path in files:\n",
    "                f = open(file_path, \"rb\")\n",
    "                files_to_upload.append((\"files\", (Path(file_path).name, f)))\n",
    "        \n",
    "        try:\n",
    "            r = httpx.post(f\"{self.base_url}/v1/chat/completions\", data=data,\n",
    "                          files=files_to_upload if files_to_upload else None,\n",
    "                          headers=self._headers(), timeout=self.timeout)\n",
    "            r.raise_for_status()\n",
    "            result = r.json()\n",
    "            return result[\"choices\"][0][\"message\"][\"content\"], result[\"x_session_id\"]\n",
    "        finally:\n",
    "            for _, (_, f) in files_to_upload:\n",
    "                f.close()\n",
    "\n",
    "    def chat_continue(self, model: str, session_id: str, user_message: str, \n",
    "                     agent_type: str = \"auto\", files: list = None):\n",
    "        messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "        data = {\"model\": model, \"messages\": json.dumps(messages), \n",
    "                \"session_id\": session_id, \"agent_type\": agent_type}\n",
    "        \n",
    "        files_to_upload = []\n",
    "        if files:\n",
    "            for file_path in files:\n",
    "                f = open(file_path, \"rb\")\n",
    "                files_to_upload.append((\"files\", (Path(file_path).name, f)))\n",
    "        \n",
    "        try:\n",
    "            r = httpx.post(f\"{self.base_url}/v1/chat/completions\", data=data,\n",
    "                          files=files_to_upload if files_to_upload else None,\n",
    "                          headers=self._headers(), timeout=self.timeout)\n",
    "            r.raise_for_status()\n",
    "            result = r.json()\n",
    "            return result[\"choices\"][0][\"message\"][\"content\"], result[\"x_session_id\"]\n",
    "        finally:\n",
    "            for _, (_, f) in files_to_upload:\n",
    "                f.close()\n",
    "\n",
    "    def get_session_artifacts(self, session_id: str):\n",
    "        \"\"\"Get list of files generated during the session\"\"\"\n",
    "        r = httpx.get(f\"{self.base_url}/api/chat/sessions/{session_id}/artifacts\",\n",
    "                     headers=self._headers(), timeout=10.0)\n",
    "        r.raise_for_status()\n",
    "        return r.json()\n",
    "\n",
    "# Configuration\n",
    "API_BASE_URL = 'http://localhost:1007'\n",
    "USERNAME = \"leesihun\"\n",
    "PASSWORD = \"s.hun.lee\"\n",
    "\n",
    "# Initialize and login\n",
    "client = LLMApiClient(API_BASE_URL, timeout=3600.0)\n",
    "client.login(USERNAME, PASSWORD)\n",
    "models = client.list_models()\n",
    "MODEL = models[\"data\"][0][\"id\"]\n",
    "\n",
    "print(f\"✓ Logged in as: {USERNAME}\")\n",
    "print(f\"✓ Using model: {MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your data files\n",
    "stats_paths = [\n",
    "    Path(\"B8_1021_stats.json\"),\n",
    "    Path(\"B8_1027_stats.json\"),\n",
    "]\n",
    "\n",
    "# Verify files exist\n",
    "print(f\"Configured {len(stats_paths)} data file(s):\\n\")\n",
    "for i, path in enumerate(stats_paths, 1):\n",
    "    if path.exists():\n",
    "        size_kb = path.stat().st_size / 1024\n",
    "        print(f\"  [{i}] {path.name} ({size_kb:.1f} KB) - ✓\")\n",
    "    else:\n",
    "        print(f\"  [{i}] {path.name} - ✗ NOT FOUND\")\n",
    "\n",
    "file_paths_str = [str(p) for p in stats_paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Phase 1: Data Analysis\n",
    "\n",
    "The AI will analyze your data and identify key patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified prompt - 90% shorter than v1\n",
    "analysis_prompt = f\"\"\"\n",
    "Analyze {len(stats_paths)} warpage measurement JSON files attached.\n",
    "\n",
    "**Data Structure:**\n",
    "- Each file has warpage statistics per PCB board\n",
    "- Fields: min, max, range (warpage value), mean, median, std, skewness, kurtosis\n",
    "- PCA values (pc1, pc2) calculated within each source_pdf\n",
    "- Filenames contain acquisition date/time (e.g., 20251027121034 = 2025.10.27 12:10:34)\n",
    "\n",
    "**Tasks:**\n",
    "1. Calculate overall statistics (mean, std, min, max across all files)\n",
    "2. Identify PCA-based outliers using pc1, pc2 values\n",
    "3. Compare production dates - which is better quality and why?\n",
    "4. List specific outlier filenames with reasons\n",
    "5. Save results to numpy array locally\n",
    "\n",
    "**Required Output:**\n",
    "- Total measurements count\n",
    "- Outlier list with full filenames\n",
    "- Production date comparison (winner + reason)\n",
    "- Key concerns or patterns\n",
    "\n",
    "THINK HARD\n",
    "\"\"\"\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE 1: DATA ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "start = time.time()\n",
    "analysis_result, session_id = client.chat_new(\n",
    "    MODEL, analysis_prompt, agent_type=\"auto\", files=file_paths_str\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Analysis completed in {time.time() - start:.1f}s\\n\")\n",
    "print(\"=\" * 80)\n",
    "display(Latex(analysis_result))\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Phase 2: Generate Visualizations\n",
    "\n",
    "**Key:** AI reuses Phase 1 findings from conversation memory (not raw files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified prompt using phase handoff pattern\n",
    "viz_prompt = f\"\"\"\n",
    "**PRIORITY: Use your Phase 1 analysis from conversation memory.**\n",
    "\n",
    "In Phase 1, you already:\n",
    "- Analyzed {len(stats_paths)} datasets and loaded all data\n",
    "- Identified PCA outliers with pc1, pc2 values\n",
    "- Compared production dates\n",
    "- Listed specific outlier filenames\n",
    "\n",
    "**DO NOT re-analyze raw files. Use your Phase 1 findings.**\n",
    "Files attached are ONLY for verification if needed.\n",
    "\n",
    "**Task:** Create visualizations and classify outliers\n",
    "\n",
    "**Outlier Classification:**\n",
    "- **BAD outliers:** High mean/std/range (critical quality issues)\n",
    "- **GOOD outliers:** Unusual PCA position but acceptable metrics\n",
    "- **Normal:** Within PCA cluster, standard metrics\n",
    "\n",
    "**Required Charts** (save to temp_charts/):\n",
    "1. `pca_outliers_classified.png` - PC1 vs PC2 scatter (Blue=normal, Orange=good outlier, RED=bad outlier)\n",
    "2. `bad_outliers_detail.png` - Bar chart comparing bad outliers vs average\n",
    "3. `production_comparison.png` - Production date quality comparison\n",
    "4. Additional charts as appropriate (distributions, trends, control charts, etc.)\n",
    "\n",
    "**Style:** 300 DPI, seaborn whitegrid, professional colors\n",
    "\n",
    "**Required Output:**\n",
    "- List of generated chart files\n",
    "- Bad outlier summary (file IDs + reasons)\n",
    "- Production date insights\n",
    "\n",
    "THINK HARD\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE 2: VISUALIZATION GENERATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "start = time.time()\n",
    "viz_result, _ = client.chat_continue(\n",
    "    MODEL, session_id, viz_prompt, agent_type=\"auto\", files=file_paths_str\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Visualizations completed in {time.time() - start:.1f}s\\n\")\n",
    "print(\"=\" * 80)\n",
    "display(Latex(viz_result))\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Phase 3: PowerPoint Assembly\n",
    "\n",
    "**Key:** AI uses Phase 1 & 2 findings from conversation memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get total file count\n",
    "total_files = 0\n",
    "for path in stats_paths:\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        total_files += len(data.get('files', []))\n",
    "\n",
    "# Simplified prompt using phase handoff\n",
    "pptx_prompt = f\"\"\"\n",
    "**PRIORITY: Use Phase 1 & 2 findings from conversation memory.**\n",
    "\n",
    "You have:\n",
    "- Phase 1: Statistics, outlier IDs, production date comparison\n",
    "- Phase 2: Charts in temp_charts/, bad outlier classifications\n",
    "\n",
    "**DO NOT re-analyze raw files. Use conversation context.**\n",
    "Files attached are ONLY for verification if needed.\n",
    "\n",
    "**Task:** Create PowerPoint using python-pptx\n",
    "\n",
    "**Presentation:**\n",
    "- Title: \"Warpage Analysis Report\"\n",
    "- Subtitle: \"Analysis of {total_files} Measurements ({len(stats_paths)} Production Dates)\"\n",
    "- Filename: `Warpage_Report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pptx`\n",
    "- Size: 10 x 7.5 inches\n",
    "\n",
    "**Slide Structure:**\n",
    "\n",
    "1. **Title Slide** - Report title, subtitle, date\n",
    "\n",
    "2. **Executive Summary (CRITICAL)** - RED ALERT BOX with:\n",
    "   - Bad outliers list from Phase 2 (file IDs + reasons)\n",
    "   - 4 metric cards: Total measurements, Better production date, Bad outlier count, Avg mean\n",
    "   - Production date analysis from Phase 1\n",
    "   - Key findings\n",
    "\n",
    "3. **PCA Outlier Classification** - pca_outliers_classified.png + color legend\n",
    "\n",
    "4. **Bad Outlier Details** - bad_outliers_detail.png + explanations from Phase 2\n",
    "\n",
    "5. **Production Comparison** - production_comparison.png + insights from Phase 1\n",
    "\n",
    "6-N. **Additional Charts** - Auto-discover ALL remaining PNG files in temp_charts/\n",
    "\n",
    "N+1. **Recommendations** - Immediate actions, quality improvements, monitoring plan\n",
    "\n",
    "**Style:** Use blank layout (index 6), professional colors (Blue #1f77b4, Red #d62728, Orange #ff7f0e, Green #2ca02c)\n",
    "\n",
    "**CRITICAL:** Extract bad outliers and metrics from Phase 1 & 2 conversation context (not from files)\n",
    "\n",
    "\n",
    "THINK HARD\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE 3: POWERPOINT ASSEMBLY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "start = time.time()\n",
    "pptx_result, _ = client.chat_continue(\n",
    "    MODEL, session_id, pptx_prompt, agent_type=\"auto\", files=file_paths_str\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ PowerPoint completed in {time.time() - start:.1f}s\\n\")\n",
    "print(\"=\" * 80)\n",
    "display(Latex(pptx_result))\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary & Results\n",
    "\n",
    "View generated artifacts using the new session artifacts API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"REPORT GENERATION COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Option 1: Use new artifacts API (recommended)\n",
    "try:\n",
    "    artifacts_data = client.get_session_artifacts(session_id)\n",
    "    artifacts = artifacts_data.get(\"artifacts\", [])\n",
    "    \n",
    "    print(f\"\\n✓ Found {len(artifacts)} generated files via API:\\n\")\n",
    "    \n",
    "    pptx_files = [a for a in artifacts if a['extension'] == '.pptx']\n",
    "    chart_files = [a for a in artifacts if a['extension'] == '.png']\n",
    "    \n",
    "    if pptx_files:\n",
    "        print(\"PowerPoint Presentations:\")\n",
    "        for pptx in pptx_files:\n",
    "            print(f\"  - {pptx['filename']} ({pptx['size_kb']:.2f} KB)\")\n",
    "    \n",
    "    if chart_files:\n",
    "        print(f\"\\nCharts ({len(chart_files)}):\")\n",
    "        for chart in chart_files:\n",
    "            print(f\"  - {chart['filename']}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"\\n⚠ Artifacts API unavailable: {e}\")\n",
    "    print(\"Falling back to manual file discovery...\\n\")\n",
    "    \n",
    "    # Option 2: Manual discovery (fallback)\n",
    "    pptx_files = sorted(glob.glob(\"Warpage_Report_*.pptx\"), reverse=True)\n",
    "    if pptx_files:\n",
    "        latest = pptx_files[0]\n",
    "        size_kb = Path(latest).stat().st_size / 1024\n",
    "        print(f\"PowerPoint: {latest} ({size_kb:.2f} KB)\")\n",
    "    \n",
    "    temp_charts = Path(\"temp_charts\")\n",
    "    if temp_charts.exists():\n",
    "        charts = list(temp_charts.glob(\"*.png\"))\n",
    "        print(f\"\\nCharts ({len(charts)}):\")\n",
    "        for chart in sorted(charts):\n",
    "            print(f\"  - {chart.name}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"WHAT CHANGED IN V2\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "✓ Prompts 90% shorter (500 lines → 50 lines)\n",
    "✓ Conversation context reuse (phases reference prior work)\n",
    "✓ Files as fallback only (no redundant re-processing)\n",
    "✓ New artifacts API (programmatic file discovery)\n",
    "✓ Clearer phase handoff instructions\n",
    "\n",
    "Same output quality, much simpler orchestration!\n",
    "\"\"\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## About This Notebook (v2)\n",
    "\n",
    "**Key Improvements:**\n",
    "- **Conversation context reuse:** Each phase explicitly references previous work\n",
    "- **Files as fallback:** Data files only for verification, not re-processing  \n",
    "- **90% shorter prompts:** From ~500 lines to ~50 lines total\n",
    "- **Artifacts API:** Programmatic discovery of generated files\n",
    "\n",
    "**Architecture Pattern:**\n",
    "```\n",
    "Phase 1: Analyze(files) → findings stored in conversation\n",
    "           ↓\n",
    "Phase 2: Visualize(Phase 1 findings) → charts + classification\n",
    "           ↓\n",
    "Phase 3: Build PPTX(Phase 1 + Phase 2 findings) → presentation\n",
    "```\n",
    "\n",
    "Each phase prioritizes **conversation memory** over raw file re-processing.\n",
    "\n",
    "---\n",
    "\n",
    "**Version:** 2.0.0 (Simplified)  \n",
    "**Last Updated:** January 2025"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
