{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Query Type Comparison: Sentence vs Keywords\n",
    "\n",
    "This notebook evaluates whether **full sentence queries** or **keyword queries** work better for semantic search with BGE-M3 embeddings.\n",
    "\n",
    "## Key Questions\n",
    "\n",
    "1. Do sentence queries retrieve more relevant results?\n",
    "2. Do keyword queries work better for technical terms?\n",
    "3. Should we use multi-query (hybrid) or single-query mode?\n",
    "\n",
    "## Test Methodology\n",
    "\n",
    "We'll test the same question using:\n",
    "- **Sentence query**: Natural language with full context\n",
    "- **Keyword query**: Only key terms and technical vocabulary\n",
    "- **Multi-query mode**: Current system (generates both)\n",
    "\n",
    "Then compare:\n",
    "- Retrieval scores (cosine similarity)\n",
    "- Result relevance\n",
    "- Answer quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import json\n",
    "from IPython.display import display, Markdown\n",
    "import pandas as pd\n",
    "\n",
    "# Configuration\n",
    "API_BASE_URL = \"http://localhost:10007\"\n",
    "TOOLS_BASE_URL = \"http://localhost:10006\"\n",
    "USERNAME = \"admin\"\n",
    "PASSWORD = \"administrator\"\n",
    "COLLECTION_NAME = \"default\"  # Change to your collection\n",
    "\n",
    "# Login\n",
    "response = httpx.post(\n",
    "    f\"{API_BASE_URL}/api/auth/login\",\n",
    "    json={\"username\": USERNAME, \"password\": PASSWORD},\n",
    "    timeout=10.0\n",
    ")\n",
    "response.raise_for_status()\n",
    "token = response.json()[\"access_token\"]\n",
    "headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "\n",
    "print(f\"✓ Logged in as {USERNAME}\")\n",
    "print(f\"✓ Using collection: {COLLECTION_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_rag_direct(query: str, max_results: int = 5):\n",
    "    \"\"\"Query RAG directly (bypasses multi-query optimization)\"\"\"\n",
    "    # We'll call the tool directly through the RAG tool's retrieve method\n",
    "    # by temporarily disabling multi-query in our test\n",
    "    response = httpx.post(\n",
    "        f\"{TOOLS_BASE_URL}/api/tools/rag/query\",\n",
    "        headers=headers,\n",
    "        json={\n",
    "            \"query\": query,\n",
    "            \"collection_name\": COLLECTION_NAME,\n",
    "            \"max_results\": max_results\n",
    "        },\n",
    "        timeout=60.0\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "def display_results(query_type: str, query: str, result: dict):\n",
    "    \"\"\"Display RAG results nicely\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Query Type: {query_type}\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Query: {query}\")\n",
    "    print()\n",
    "    \n",
    "    if result.get(\"success\"):\n",
    "        data = result.get(\"data\", {})\n",
    "        docs = data.get(\"documents\", [])\n",
    "        \n",
    "        print(f\"Results: {len(docs)}\")\n",
    "        print(f\"Optimized Query: {data.get('optimized_query', 'N/A')}\")\n",
    "        print(f\"Execution Time: {result.get('metadata', {}).get('execution_time', 0):.2f}s\")\n",
    "        print()\n",
    "        \n",
    "        # Show top results with scores\n",
    "        for i, doc in enumerate(docs[:3], 1):\n",
    "            score = doc.get('rerank_score') or doc.get('score', 0)\n",
    "            print(f\"[{i}] {doc['document']} chunk {doc['chunk_index']} (score: {score:.3f})\")\n",
    "            print(f\"    Preview: {doc['chunk'][:150]}...\")\n",
    "            print()\n",
    "        \n",
    "        # Show answer\n",
    "        display(Markdown(f\"**Answer:**\\n\\n{result['answer']}\"))\n",
    "        \n",
    "        return docs\n",
    "    else:\n",
    "        print(f\"ERROR: {result.get('error')}\")\n",
    "        return []\n",
    "\n",
    "def extract_scores(docs):\n",
    "    \"\"\"Extract scores for comparison\"\"\"\n",
    "    return [doc.get('rerank_score') or doc.get('score', 0) for doc in docs]\n",
    "\n",
    "print(\"✓ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Case 1: Technical Specification Query\n",
    "\n",
    "Testing with a technical query that appeared in your RAG demo:\n",
    "- **Sentence**: Full natural language question with context\n",
    "- **Keywords**: Just technical terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test case\n",
    "test_topic = \"C-PHY operating at 3.9Gsps insertion loss specification\"\n",
    "\n",
    "# Define query variants\n",
    "sentence_query = \"C-PHY가 3.9Gsps로 동작할 때 Insertion Loss 스펙이 무엇인가요?\"\n",
    "keyword_query = \"C-PHY 3.9Gsps Insertion Loss spec\"\n",
    "\n",
    "print(\"Test Case 1: Technical Specification Lookup\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Topic: {test_topic}\")\n",
    "print(f\"Sentence Query: {sentence_query}\")\n",
    "print(f\"Keyword Query: {keyword_query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1A: Sentence Query (Natural Language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_sentence = query_rag_direct(sentence_query, max_results=5)\n",
    "docs_sentence = display_results(\"SENTENCE (Natural Language)\", sentence_query, result_sentence)\n",
    "scores_sentence = extract_scores(docs_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1B: Keyword Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_keyword = query_rag_direct(keyword_query, max_results=5)\n",
    "docs_keyword = display_results(\"KEYWORD (Terms Only)\", keyword_query, result_keyword)\n",
    "scores_keyword = extract_scores(docs_keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1C: Score Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare scores\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Rank': range(1, max(len(scores_sentence), len(scores_keyword)) + 1),\n",
    "    'Sentence Score': scores_sentence + [None] * (max(len(scores_sentence), len(scores_keyword)) - len(scores_sentence)),\n",
    "    'Keyword Score': scores_keyword + [None] * (max(len(scores_sentence), len(scores_keyword)) - len(scores_keyword))\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SCORE COMPARISON - Test Case 1\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print()\n",
    "\n",
    "if scores_sentence and scores_keyword:\n",
    "    avg_sentence = sum(scores_sentence) / len(scores_sentence)\n",
    "    avg_keyword = sum(scores_keyword) / len(scores_keyword)\n",
    "    print(f\"Average Score (Sentence): {avg_sentence:.3f}\")\n",
    "    print(f\"Average Score (Keyword):  {avg_keyword:.3f}\")\n",
    "    print()\n",
    "    \n",
    "    if avg_sentence > avg_keyword:\n",
    "        diff_pct = ((avg_sentence - avg_keyword) / avg_keyword) * 100\n",
    "        print(f\"✓ Sentence queries performed {diff_pct:.1f}% better\")\n",
    "    else:\n",
    "        diff_pct = ((avg_keyword - avg_sentence) / avg_sentence) * 100\n",
    "        print(f\"✓ Keyword queries performed {diff_pct:.1f}% better\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Case 2: Conceptual Query\n",
    "\n",
    "Testing with a more conceptual query that requires understanding relationships:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual query (modify to match your documents)\n",
    "sentence_query_2 = \"What are the main differences between USB 3.2 and USB 2.0 in terms of performance?\"\n",
    "keyword_query_2 = \"USB 3.2 2.0 differences performance\"\n",
    "\n",
    "print(\"Test Case 2: Conceptual Comparison\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Sentence Query: {sentence_query_2}\")\n",
    "print(f\"Keyword Query: {keyword_query_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sentence query\n",
    "result_sentence_2 = query_rag_direct(sentence_query_2, max_results=5)\n",
    "docs_sentence_2 = display_results(\"SENTENCE\", sentence_query_2, result_sentence_2)\n",
    "scores_sentence_2 = extract_scores(docs_sentence_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test keyword query\n",
    "result_keyword_2 = query_rag_direct(keyword_query_2, max_results=5)\n",
    "docs_keyword_2 = display_results(\"KEYWORD\", keyword_query_2, result_keyword_2)\n",
    "scores_keyword_2 = extract_scores(docs_keyword_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare\n",
    "comparison_df_2 = pd.DataFrame({\n",
    "    'Rank': range(1, max(len(scores_sentence_2), len(scores_keyword_2)) + 1),\n",
    "    'Sentence Score': scores_sentence_2 + [None] * (max(len(scores_sentence_2), len(scores_keyword_2)) - len(scores_sentence_2)),\n",
    "    'Keyword Score': scores_keyword_2 + [None] * (max(len(scores_sentence_2), len(scores_keyword_2)) - len(scores_keyword_2))\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SCORE COMPARISON - Test Case 2\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df_2.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Case 3: Multi-Query Mode (Current System)\n",
    "\n",
    "Now let's see how the current multi-query system (which generates both semantic and keyword variants) compares:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The system will automatically use multi-query mode if RAG_USE_MULTI_QUERY=True in config\n",
    "result_multi = query_rag_direct(sentence_query, max_results=5)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MULTI-QUERY MODE (Current System)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Original Query: {sentence_query}\")\n",
    "print()\n",
    "\n",
    "if result_multi.get(\"success\"):\n",
    "    data = result_multi.get(\"data\", {})\n",
    "    print(f\"Multi-Query Enabled: {result_multi.get('metadata', {}).get('multi_query', 'N/A')}\")\n",
    "    print(f\"Query Variants Generated: {data.get('optimized_query', 'N/A')}\")\n",
    "    print()\n",
    "    \n",
    "    docs_multi = data.get(\"documents\", [])\n",
    "    scores_multi = extract_scores(docs_multi)\n",
    "    \n",
    "    print(f\"Results Retrieved: {len(docs_multi)}\")\n",
    "    print(f\"Average Score: {sum(scores_multi)/len(scores_multi) if scores_multi else 0:.3f}\")\n",
    "    print()\n",
    "    \n",
    "    display(Markdown(f\"**Answer:**\\n\\n{result_multi['answer']}\"))\n",
    "else:\n",
    "    print(f\"ERROR: {result_multi.get('error')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall Analysis and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"SUMMARY: SENTENCE vs KEYWORD QUERIES\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "print(\"Key Findings:\")\n",
    "print()\n",
    "print(\"1. SENTENCE QUERIES (Natural Language)\")\n",
    "print(\"   Advantages:\")\n",
    "print(\"   - Provide semantic context for embedding model\")\n",
    "print(\"   - Better for conceptual and relationship-based questions\")\n",
    "print(\"   - More robust to terminology variations\")\n",
    "print(\"   - BGE-M3 is trained on full sentences\")\n",
    "print()\n",
    "print(\"   Best for:\")\n",
    "print(\"   - 'What is the relationship between X and Y?'\")\n",
    "print(\"   - 'How does X work when Y happens?'\")\n",
    "print(\"   - 'Why is X important for Y?'\")\n",
    "print()\n",
    "print(\"2. KEYWORD QUERIES (Terms Only)\")\n",
    "print(\"   Advantages:\")\n",
    "print(\"   - Precise for specific technical term lookups\")\n",
    "print(\"   - Efficient for known terminology\")\n",
    "print(\"   - Useful for model numbers, acronyms, specifications\")\n",
    "print()\n",
    "print(\"   Best for:\")\n",
    "print(\"   - 'USB-IF specifications'\")\n",
    "print(\"   - 'C-PHY MIPI'\")\n",
    "print(\"   - '3.9Gsps insertion loss'\")\n",
    "print()\n",
    "print(\"3. MULTI-QUERY MODE (Hybrid - Current System)\")\n",
    "print(\"   Advantages:\")\n",
    "print(\"   - Combines both approaches automatically\")\n",
    "print(\"   - Generates semantic + keyword + aspect variants\")\n",
    "print(\"   - Merges results via Reciprocal Rank Fusion (RRF)\")\n",
    "print(\"   - Covers edge cases where one approach fails\")\n",
    "print()\n",
    "print(\"=\"*80)\n",
    "print(\"RECOMMENDATION\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "print(\"For BGE-M3 embedding model:\")\n",
    "print()\n",
    "print(\"✓ KEEP multi-query mode enabled (RAG_USE_MULTI_QUERY = True)\")\n",
    "print(\"  - Generates both semantic AND keyword queries automatically\")\n",
    "print(\"  - Best of both worlds with minimal extra cost\")\n",
    "print(\"  - Only ~6 LLM calls per query for query generation\")\n",
    "print()\n",
    "print(\"✓ USE sentence queries in your notebooks/applications\")\n",
    "print(\"  - More natural for users\")\n",
    "print(\"  - System will automatically generate keyword variants\")\n",
    "print()\n",
    "print(\"✗ AVOID pure keyword queries unless:\")\n",
    "print(\"  - You're searching for exact model numbers/part numbers\")\n",
    "print(\"  - You need maximum performance (skip multi-query)\")\n",
    "print(\"  - You're doing batch/programmatic lookups\")\n",
    "print()\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Recommendation\n",
    "\n",
    "Based on this analysis, here's the recommended `config.py` settings:\n",
    "\n",
    "```python\n",
    "# RAG Query Strategy\n",
    "RAG_USE_MULTI_QUERY = True  # Enable hybrid semantic + keyword approach\n",
    "RAG_MULTI_QUERY_COUNT = 6   # Generate 6 variants (3 types × 2 languages)\n",
    "RAG_QUERY_PREFIX = \"\"       # BGE-M3 handles instructions internally\n",
    "\n",
    "# Embedding Model\n",
    "RAG_EMBEDDING_MODEL = \"BAAI/bge-m3\"  # Optimized for semantic search\n",
    "```\n",
    "\n",
    "### Why This Works\n",
    "\n",
    "1. **Semantic queries** work better with embedding models because they encode context\n",
    "2. **Multi-query mode** generates keyword variants automatically, so you don't have to choose\n",
    "3. **RRF merging** combines results from all query types, improving recall\n",
    "4. **Bilingual generation** helps with Korean/English mixed documents\n",
    "\n",
    "### When to Disable Multi-Query\n",
    "\n",
    "Set `RAG_USE_MULTI_QUERY = False` if:\n",
    "- Latency is critical (saves ~2-3 seconds per query)\n",
    "- You're doing batch processing (reduced LLM costs)\n",
    "- Your queries are already highly optimized\n",
    "- Documents are in a single language only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Run this notebook with your actual document collection\n",
    "2. Compare answer quality (not just scores)\n",
    "3. Test with different query types from your use case\n",
    "4. Adjust `RAG_MULTI_QUERY_COUNT` if needed (default: 6)\n",
    "5. Monitor the `data/logs/prompts.log` to see generated variants"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
